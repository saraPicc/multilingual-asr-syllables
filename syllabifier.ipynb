{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import pickle as pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to work on the transcriptions, so we can start wither from the pkl files where the phonetic and phonological transcriptions are stored (see Notebook multilingual_corpis.ipynb) or from the pkl file of the multilingual corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files with transcriptions \n",
    "\n",
    "# set the directory\n",
    "\n",
    "%cd /mypath/transcriptions\n",
    "\n",
    "# IT\n",
    "with open('fix_itCV_df.pkl', 'rb') as file:\n",
    "  IT_train_df, IT_test_df, IT_val_df = pkl.load(file)\n",
    "\n",
    "  # ES\n",
    "with open('fix_esCV_df.pkl', 'rb') as file:\n",
    "  ES_train_df, ES_test_df, ES_val_df = pkl.load(file)\n",
    "\n",
    "# FR\n",
    "with open('fix_frCV_df.pkl', 'rb') as file:\n",
    "  FR_train_df, FR_test_df, FR_val_df = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate datasets\n",
    "\n",
    "ITdataset = pd.concat([IT_train_df, IT_test_df, IT_val_df],  ignore_index=True)\n",
    "\n",
    "ESdataset = pd.concat([ES_train_df, ES_test_df, ES_val_df],  ignore_index=True)\n",
    "\n",
    "FRdataset = pd.concat([FR_train_df, FR_test_df, FR_val_df],  ignore_index=True)\n",
    "\n",
    "MLdataset = pd.concat([ITdataset, ESdataset, FRdataset])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the list of single phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "34\n",
      "46\n",
      "size mlt phonemes 53\n",
      "['l', 'A', 'n', 'b', 'R', 'g', 'tts', 'ddz', 'p', 'T', 'x', 'o~', '9', 'G', '2', '@', 'z', 'a', 'y', 'e~', 'e', 't', 'u', 'Z', 'B', 'dz', 'ts', 'r', 'ttS', 'H', 'k', 'D', 'L', 'S', 'a~', 'm', 'j', 'J', 'N', 'dZ', 'X', 's', '9~', 'w', 'v', 'o', 'tS', 'O', 'i', 'd', 'ddZ', 'E', 'f']\n"
     ]
    }
   ],
   "source": [
    "# SINGLE PHONEMES\n",
    "\n",
    "ITphoClasses = {'a': 0, 'e': 0, 'o': 0, 'O': 0, 'E': 0,'i': 0, 'u': 0,  # vowels\n",
    "                    'j': 1, 'w': 1,                                                 # glides\n",
    "                    'r': 2,                                                         # rhotic\n",
    "                    'l': 3, 'L': 3,                                                 # lateral\n",
    "                    'm': 4, 'n': 4, 'J': 4, 'N': 4,                                 # nasals\n",
    "                    's': 5, 'z': 5,                                                 # fricatives I (sibilants)\n",
    "                    'v': 6, 'f': 6,                                                 # fricatives II\n",
    "                    'Z': 7, 'S': 7,                                                 # frivatives III\n",
    "                    'dz': 8, 'dZ': 8, 'tts': 8, 'ttS': 8, 'ts': 8, 'tS': 8, 'ddz': 8, 'ddZ': 8,  # affriactes + affr geminates\n",
    "                    'b': 9,'d': 9, 'g': 9, 'k': 9, 'p': 9, 't': 9}\n",
    "\n",
    "\n",
    "ESphoClasses = {'e': 0, 'i': 0, 'o': 0,'u': 0, 'a': 0,\n",
    "                'j': 1, 'w': 1,\n",
    "                'r': 2,\n",
    "                'l': 3,'L': 3,\n",
    "                'm': 4, 'n': 4, 'J': 4, 'N': 4,\n",
    "                's': 5,\n",
    "                'v': 6, 'B': 6, 'D': 6, 'G': 6, 'Z': 6, 'S': 6, 'T': 6, 'x': 6,\n",
    "                'f': 7,\n",
    "                'ddZ': 8, 'dZ': 8, 'tS': 8, 'ttS': 8,\n",
    "                'b': 9, 'd': 9, 'g': 9, 'k': 9, 'p': 9,'t': 9}\n",
    "\n",
    "\n",
    "FRphoClasses = {'e': 0, 'i': 0, 'o': 0,'u': 0, 'a': 0, '@': 0, 'E':0, 'O':0, 'y':0, '2':0, '9':0,\n",
    "                  'A':0, 'e~':0, 'a~':0, 'o~':0, '9~':0,\n",
    "                   'j': 1, 'w': 1, 'H':1,\n",
    "                   'R': 2,\n",
    "                   'l': 3,\n",
    "                   'm': 4, 'n': 4, 'J': 4, 'N': 4,\n",
    "                   's': 5,\n",
    "                   'v': 6, 'f': 6, 'z': 6, 'X': 6, 'Z': 6, 'S': 6,\n",
    "                   'ddz': 8, 'dz': 8, 'ts': 8, 'tts': 8, 'ddZ' : 8, 'dZ' : 8, 'tS' : 8, 'ttS' : 8,\n",
    "                   'b': 9, 'd': 9, 'g': 9, 'k': 9, 'p': 9,'t': 9}\n",
    "\n",
    "segmIT_list = [k for k in ITphoClasses.keys()]\n",
    "print(len(segmIT_list))\n",
    "segmES_list = [k for k in ESphoClasses.keys()]\n",
    "print(len(segmES_list))\n",
    "segmFR_list = [k for k in FRphoClasses.keys()]\n",
    "print(len(segmFR_list))\n",
    "\n",
    "segmMLlist = list(set(segmIT_list + segmES_list + segmFR_list))\n",
    "print('size mlt phonemes', len(segmMLlist))\n",
    "print(segmMLlist)\n",
    "\n",
    "\n",
    "MLphoClasses = {'e': 0, 'i': 0, 'o': 0,'u': 0, 'a': 0, '@': 0, 'E':0, 'O':0, 'y':0, '2':0, '9':0, # vowels\n",
    "                 'A':0, 'e~':0, 'a~':0, 'o~':0, '9~':0, '~': 11,\n",
    "                 'j': 1, 'w': 1, 'H':1,\n",
    "                 'r': 2, 'R': 2,\n",
    "                 'l': 3,'L': 3,\n",
    "                 'm': 4, 'n': 4, 'J': 4, 'N': 4,\n",
    "                 's': 5, 'z': 5,\n",
    "                 'v': 6, 'f': 6,\n",
    "                 'B': 7, 'D': 7, 'G': 7, 'Z': 7, 'S': 7, 'T': 7, 'x': 7, 'X': 7,\n",
    "                 'ddz': 8, 'dz': 8, 'ts': 8, 'tts': 8, 'ddZ' : 8, 'dZ' : 8, 'tS' : 8, 'ttS' : 8,\n",
    "                 'b': 9, 'd': 9, 'g': 9, 'k': 9, 'p': 9,'t': 9}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual syllabifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyllabifierML(text):\n",
    "\n",
    "  \"\"\"Takes a transcribed sentence as an input and returns it syllabified. Words are separated by pipes and \n",
    "  syllables within each word are separated by white spaces\"\"\"\n",
    "\n",
    "  MLTnucleus = ['a', 'e', 'o', 'i', 'u', '@', 'E', 'O', 'y', '2', '9', 'A', 'e~', 'a~', 'o~', '9~']\n",
    "\n",
    "  MLTphoClasses = {'e': 0, 'i': 0, 'o': 0,'u': 0, 'a': 0, '@': 0, 'E':0, 'O':0, 'y':0, '2':0, '9':0, # vowels\n",
    "                 'A':0, 'e~':0, 'a~':0, 'o~':0, '9~':0, '~': 11,\n",
    "                 'j': 1, 'w': 1, 'H':1,\n",
    "                 'r': 2, 'R': 2,\n",
    "                 'l': 3,'L': 3,\n",
    "                 'm': 4, 'n': 4, 'J': 4, 'N': 4,\n",
    "                 's': 5, 'z': 5,\n",
    "                 'v': 6, 'f': 6,\n",
    "                 'B': 7, 'D': 7, 'G': 7, 'Z': 7, 'S': 7, 'T': 7, 'x': 7, 'X': 7,\n",
    "                 'ddz': 8, 'dz': 8, 'ts': 8, 'tts': 8, 'ddZ' : 8, 'dZ' : 8, 'tS' : 8, 'ttS' : 8,\n",
    "                 'b': 9, 'd': 9, 'g': 9, 'k': 9, 'p': 9,'t': 9}\n",
    "\n",
    "\n",
    "  # set the word separator\n",
    "\n",
    "  listlist = [ ]\n",
    "\n",
    "  # print('splitting sent') # words in list to avoid syllabification outside word boundaries\n",
    "\n",
    "  phone_delimiter_token=\" \"\n",
    "  word_delimiter_token=\" | \"\n",
    "\n",
    "  word_delimiter_tok = word_delimiter_token #+ ' '\n",
    "  pho_delimiter_tok = phone_delimiter_token\n",
    "\n",
    "  #### EXAMPLE ------------------- 'nessun lavoro sErjo di applikattsjone E denari in abbondantsa'\n",
    "\n",
    "  splitted_sent = text.split(pho_delimiter_tok)\n",
    "  # print('splitted_sent:', splitted_sent)  ### --------------- ['nessun', 'lavoro', 'sErjo', 'di', 'applikattsjone', 'E', 'denari', 'in', 'abbondantsa']\n",
    "\n",
    "  raw_MOP_sent = [ ] # splits after each vowel\n",
    "\n",
    "  for item in splitted_sent: # sillabification starts here\n",
    "\n",
    "    raw_MOP_w = \"\"\n",
    "\n",
    "    for seg in item:\n",
    "      if seg in MLTnucleus:\n",
    "        raw_MOP_w += seg + phone_delimiter_token\n",
    "      elif seg == '~':\n",
    "        raw_MOP_w = raw_MOP_w[ :-1] + '~' + phone_delimiter_token\n",
    "\n",
    "        # print('raw_MOP~',  raw_MOP_w)\n",
    "\n",
    "      else:\n",
    "        raw_MOP_w += seg\n",
    "\n",
    "    raw_MOP_sent.append(raw_MOP_w.strip())\n",
    "\n",
    "  # print('raw MOP___: ', raw_MOP_sent)  ### --------------- ['ne ssu n', 'la vo ro', 'sE rjo', 'di', 'a ppli ka ttsjo ne', 'E', 'de na ri', 'i n', 'a bbo nda ntsa']\n",
    "\n",
    "\n",
    "  # syllables in list to check if they're valid or violate SSP\n",
    "\n",
    "  for i, rawSyl_w in enumerate(raw_MOP_sent):\n",
    "    raw_MOP_sent_list = rawSyl_w.split(phone_delimiter_token)\n",
    "    # print('word to check ', i, raw_MOP_sent_list) ### ---------------  0 ['ne', 'ssu', 'n']\n",
    "\n",
    "\n",
    "    # SSP check\n",
    "\n",
    "    \"\"\"every segment is mappend into a class tht has a numeric ID according to the sonority.\n",
    "    Onset segments of the syllables of the rough MOP tokenization are asigned with a sonority ID\n",
    "    that is appended to a list; if the list matches with one of the allowed combinatios the syllable\n",
    "    is valid (SSP = True), otherwise the problematic onsets that violates the SSP are appended as coda of the previous syllable\"\"\"\n",
    "\n",
    "    ok_MOP_sent = [ ]\n",
    "\n",
    "\n",
    "\n",
    "    SSP_allowed = [[1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], # CV\n",
    "                [1, 0, 11], [2, 0, 11], [3, 0, 11], [4, 0, 11], [5, 0, 11], [6, 0, 11],  # CV~\n",
    "                [7, 0, 11], [8, 0, 11], [9, 0, 11], [5, 1, 0, 11], [5, 1, 0],            # CV~\n",
    "                [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9], # geminates\n",
    "                [9 ,9, 3] ,[9 ,9, 2], # geminates + liquids\n",
    "                [2, 1], [3, 1], [4, 1], [5, 1], [6, 1], [7, 1], [8, 1], [9, 1],# everything + approximants\n",
    "                [5, 2], [6, 2], [7, 2], [9, 2], # fric plosives + /r/  # ------------------- ok\n",
    "                [5, 3], [6, 3], [7, 3], [9, 3],  # fric, plosives + /l/ # ------------------- ok\n",
    "                [5, 4], [7, 4], [9, 4],   # + nasali - con le plosive ci sono eccezioni tolto [8, 4] - pneumatico\n",
    "                [9, 5],  # + /s/ /z/ - con le plosive ci sono eccezioni psicologo\n",
    "                [7, 6], [7, 9, 2],  [8, 6],  # + /S/ /Z/ - con le plosive ci sono eccezioni\n",
    "                [5, 9], [5, 6],  # s impura\n",
    "                [5, 6, 1],  [5, 6, 2], [5, 6, 3], [5, 9, 1],  [5, 9, 2], [5, 9, 3], # s impura + CC --------- ok\n",
    "                [5, 6, 2, 1], [5, 6, 3, 1], [5, 9, 2, 1], [5, 9, 3, 1], # s impura + CC + glide --------- ok\n",
    "                [9, 9, 5], [9, 9, 6], [9, 9, 5, 1], [9, 9, 6, 1],\n",
    "                  [9, 5, 9], [6, 9]] # per gestire affricate\n",
    "\n",
    "\n",
    "    current_syl = ''\n",
    "\n",
    "    for i, syl in enumerate(raw_MOP_sent_list):\n",
    "      seg_class = [ ]\n",
    "      onset = syl[:-1]\n",
    "\n",
    "      if len(onset) > 1:\n",
    "        for ch in onset:\n",
    "          seg_class.append(MLTphoClasses[ch]) ###class del carattere\n",
    "\n",
    "        if seg_class in SSP_allowed:\n",
    "          MOP_ok = True\n",
    "        else:\n",
    "          MOP_ok = False\n",
    "\n",
    "      else:\n",
    "        MOP_ok = True # if the onset is only one C it is always legit\n",
    "\n",
    "      ok_MOP_sent.append([syl, MOP_ok])\n",
    "\n",
    "\n",
    "\n",
    "    # print('ok_MOP_sent: ', ok_MOP_sent)     ### ---------------  [['a', True], ['bbo', True], ['nda', False], ['ntsa', False]]\n",
    "                                            ### ---------------  0 [['ne', True], ['ssu', True], ['n', True]]\n",
    "\n",
    "\n",
    "    # fixing syl that violate the MOP\n",
    "\n",
    "\n",
    "    final_syl_sent = [ ]\n",
    "\n",
    "    current_syl = None\n",
    "\n",
    "    for i, syl_MOP in enumerate(ok_MOP_sent):\n",
    "      if syl_MOP[1] == True:\n",
    "        current_syl = syl_MOP[0]\n",
    "        final_syl_sent.append(current_syl)\n",
    "\n",
    "      if syl_MOP[1] == False:\n",
    "        try:\n",
    "          prev_syll = final_syl_sent[-1]\n",
    "          probl = syl_MOP[0][0]\n",
    "          prev_syll = prev_syll+probl\n",
    "          final_syl_sent.pop()\n",
    "          final_syl_sent.append(prev_syll)\n",
    "\n",
    "          ok_syl = syl_MOP[0].replace(probl, \"\")\n",
    "          current_syl = ok_syl\n",
    "          final_syl_sent.append(current_syl)\n",
    "\n",
    "        except IndexError: # fixes if onset is unseparable consonants(exceptions, names)\n",
    "            current_syl = syl_MOP[0]\n",
    "            final_syl_sent.append(current_syl)\n",
    "\n",
    "\n",
    "\n",
    "    if len(final_syl_sent[-1]) == 1 and final_syl_sent[-1] not in MLTnucleus: # fixes if last syllable is a consonant alone\n",
    "      probl_coda = final_syl_sent[-1]\n",
    "      try:\n",
    "        final_syl_sent.pop()\n",
    "        last_s = final_syl_sent[-1]\n",
    "        final_syl_sent.pop()\n",
    "        fixed_last = last_s + probl_coda\n",
    "        final_syl_sent.append(fixed_last)\n",
    "      except IndexError:\n",
    "        # print('i have a problem with', probl_coda)\n",
    "            if probl_coda in 'dl':  #\n",
    "              final_syl_sent.append(probl_coda) #\n",
    "            else:\n",
    "              final_syl_sent.append(probl_coda+'e') #\n",
    "\n",
    "\n",
    "    if final_syl_sent[-1] not in 'dl' and not any(substring in final_syl_sent[-1] for substring in MLTnucleus):\n",
    "\n",
    "      last_Csyl =  final_syl_sent[-1]\n",
    "\n",
    "      final_syl_sent.pop()\n",
    "      last_s = final_syl_sent[-1]\n",
    "\n",
    "      final_syl_sent.pop()\n",
    "      fixed_last = last_s + last_Csyl\n",
    "\n",
    "      final_syl_sent.append(fixed_last)\n",
    "\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "\n",
    "    syl_w_str = phone_delimiter_token.join(final_syl_sent)\n",
    "\n",
    "    listlist.append(syl_w_str+ word_delimiter_tok)\n",
    "    # listlist.append(syl_w_str+' '+ word_delimiter_tok)\n",
    "\n",
    "  text = ''.join(listlist)\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it example:   tSi | so no | tro ppE | ko ze | da | dZat Sin to | \n",
      "fr example:   o | no~ | dy | gu vER n@ ma~ | Z@ | ma~ | R@ mE | do~k | o ZuR dHi | a | la | sa ZEs | d@ | l | a sa~ ble | \n",
      "es example:   u na | BoT | fe me ni na | le | Gri ta | Dez De | le xos | \n",
      "fr example:   m@ sj2 | pOl | a | RE zo~ | il | fo | k@ | s@ la | sERv | d | Eg za~pl | \n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "\n",
    "print('it example:  ', SyllabifierML('tSi sono troppE koze da dZatSinto'))\n",
    "print('fr example:  ', SyllabifierML('o no~ dy guvERn@ma~ Z@ ma~ R@mE do~k oZuRdHi a la saZEs d@ l asa~ble'))\n",
    "print('es example:  ', SyllabifierML('una BoT femenina le Grita DezDe lexos'))\n",
    "print('fr example:  ', SyllabifierML('m@sj2 pOl a REzo~ il fo k@ s@la sERv d Egza~pl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the syllable-based vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary will contain the most frequent syllables in the multilingual corpus and all the single phonemes to compose less frequent syllables. This way we will be able to avoid data sparsity, a problem that may occurr due to the fact that we are working with a limited amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syll_in_list(sent):\n",
    "  x = sent.split()\n",
    "  return x\n",
    "\n",
    "\n",
    "def syl_count(list_syl):\n",
    "  counts = {}\n",
    "  for item in list_syl:\n",
    "    counts[item] = counts.get(item, 0) + 1\n",
    "  least_frequentSY = [ k for k, v in counts.items() if v < 3500] # by modifying this threshold we define the vocbaulary size\n",
    "\n",
    "  # print('least_frequentSY_____________', least_frequentSY[:10], len(least_frequentSY))\n",
    "  return counts, least_frequentSY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the most frequent syllables and set the vocabulary size through the frequency threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15391\n",
      "15593\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "#(1) syllabify each sentence\n",
    "MLdataset['phonl_tr'] = MLdataset['phonl_tr'].apply(SyllabifierML)\n",
    "\n",
    "#(2) each row of the dataset as a list of syllables\n",
    "MLdataset['phonl_tr'] = MLdataset['phonl_tr'].apply(syll_in_list)\n",
    "\n",
    "# (3) extraction of the whole list of syllables + clean pipe\n",
    "ML_syl_lists = [l for l in MLdataset['phonl_tr']] \n",
    "ML_syl_lists = [item for sublist in ML_syl_lists for item in sublist] # flat syl list\n",
    "MLallSyl = [ el for el in ML_syl_lists if el != '|' ] #  -no pipe\n",
    "\n",
    "\n",
    "\n",
    "# (4) syllable frequency count + less frequent syllables list\n",
    "MLsylCount, ML_leastFrSyl = syl_count(MLallSyl) \n",
    "print(len(ML_leastFrSyl))\n",
    "\n",
    "# (5) complete syllable set\n",
    "ML_syl_list = list(set(ML_syl_lists)) \n",
    "print(len(ML_syl_list))\n",
    "\n",
    "# (6) most frequent syllables \n",
    "MLmostFsyl = list(set(MLallSyl) - set(ML_leastFrSyl))\n",
    "print(len(MLmostFsyl))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': 0,\n",
       " '9': 1,\n",
       " '9~': 2,\n",
       " '@': 3,\n",
       " 'A': 4,\n",
       " 'B': 5,\n",
       " 'Ba': 6,\n",
       " 'Be': 7,\n",
       " 'Bi': 8,\n",
       " 'Bo': 9,\n",
       " 'D': 10,\n",
       " 'Da': 11,\n",
       " 'DaT': 12,\n",
       " 'De': 13,\n",
       " 'Di': 14,\n",
       " 'Do': 15,\n",
       " 'Dos': 16,\n",
       " 'E': 17,\n",
       " 'El': 18,\n",
       " 'G': 19,\n",
       " 'Ga': 20,\n",
       " 'Go': 21,\n",
       " 'H': 22,\n",
       " 'J': 23,\n",
       " 'L': 24,\n",
       " 'N': 25,\n",
       " 'O': 26,\n",
       " 'R': 27,\n",
       " 'R@': 28,\n",
       " 'RE': 29,\n",
       " 'Ra': 30,\n",
       " 'Ra~': 31,\n",
       " 'Re': 32,\n",
       " 'Ri': 33,\n",
       " 'Ro': 34,\n",
       " 'Ry': 35,\n",
       " 'S': 36,\n",
       " 'Sa': 37,\n",
       " 'Se': 38,\n",
       " 'Si': 39,\n",
       " 'T': 40,\n",
       " 'Ta': 41,\n",
       " 'Te': 42,\n",
       " 'Ti': 43,\n",
       " 'Tja': 44,\n",
       " 'Tjo': 45,\n",
       " 'Tjon': 46,\n",
       " 'X': 47,\n",
       " 'Z': 48,\n",
       " 'Z@': 49,\n",
       " 'Ze': 50,\n",
       " 'Zi': 51,\n",
       " 'Zo': 52,\n",
       " 'a': 53,\n",
       " 'al': 54,\n",
       " 'an': 55,\n",
       " 'ar': 56,\n",
       " 'a~': 57,\n",
       " 'b': 58,\n",
       " 'ba': 59,\n",
       " 'be': 60,\n",
       " 'bi': 61,\n",
       " 'bo': 62,\n",
       " 'd': 63,\n",
       " 'd@': 64,\n",
       " 'dZ': 65,\n",
       " 'da': 66,\n",
       " 'da~': 67,\n",
       " 'ddZ': 68,\n",
       " 'ddz': 69,\n",
       " 'de': 70,\n",
       " 'del': 71,\n",
       " 'di': 72,\n",
       " 'do': 73,\n",
       " 'du': 74,\n",
       " 'dy': 75,\n",
       " 'dz': 76,\n",
       " 'e': 77,\n",
       " 'el': 78,\n",
       " 'en': 79,\n",
       " 'es': 80,\n",
       " 'e~': 81,\n",
       " 'f': 82,\n",
       " 'fa': 83,\n",
       " 'fe': 84,\n",
       " 'fi': 85,\n",
       " 'for': 86,\n",
       " 'fu': 87,\n",
       " 'fwe': 88,\n",
       " 'g': 89,\n",
       " 'ga': 90,\n",
       " 'go': 91,\n",
       " 'i': 92,\n",
       " 'il': 93,\n",
       " 'im': 94,\n",
       " 'in': 95,\n",
       " 'j': 96,\n",
       " 'k': 97,\n",
       " 'k@': 98,\n",
       " 'ka': 99,\n",
       " 'kan': 100,\n",
       " 'ke': 101,\n",
       " 'ki': 102,\n",
       " 'ko': 103,\n",
       " 'kom': 104,\n",
       " 'kon': 105,\n",
       " 'ko~': 106,\n",
       " 'ku': 107,\n",
       " 'kwa': 108,\n",
       " 'kwe': 109,\n",
       " 'l': 110,\n",
       " 'l@': 111,\n",
       " 'la': 112,\n",
       " 'las': 113,\n",
       " 'le': 114,\n",
       " 'les': 115,\n",
       " 'li': 116,\n",
       " 'lla': 117,\n",
       " 'lle': 118,\n",
       " 'llo': 119,\n",
       " 'lo': 120,\n",
       " 'los': 121,\n",
       " 'lu': 122,\n",
       " 'm': 123,\n",
       " 'm@': 124,\n",
       " 'mE': 125,\n",
       " 'ma': 126,\n",
       " 'mas': 127,\n",
       " 'ma~': 128,\n",
       " 'me': 129,\n",
       " 'men': 130,\n",
       " 'mi': 131,\n",
       " 'mo': 132,\n",
       " 'mu': 133,\n",
       " 'n': 134,\n",
       " 'n@': 135,\n",
       " 'na': 136,\n",
       " 'nal': 137,\n",
       " 'ne': 138,\n",
       " 'nes': 139,\n",
       " 'ni': 140,\n",
       " 'no': 141,\n",
       " 'non': 142,\n",
       " 'nu': 143,\n",
       " 'ny': 144,\n",
       " 'o': 145,\n",
       " 'o~': 146,\n",
       " 'p': 147,\n",
       " 'pO': 148,\n",
       " 'pa': 149,\n",
       " 'paR': 150,\n",
       " 'par': 151,\n",
       " 'pe': 152,\n",
       " 'per': 153,\n",
       " 'pi': 154,\n",
       " 'po': 155,\n",
       " 'por': 156,\n",
       " 'pre': 157,\n",
       " 'pri': 158,\n",
       " 'pro': 159,\n",
       " 'pu': 160,\n",
       " 'puR': 161,\n",
       " 'r': 162,\n",
       " 'ra': 163,\n",
       " 'ran': 164,\n",
       " 're': 165,\n",
       " 'res': 166,\n",
       " 'ri': 167,\n",
       " 'rja': 168,\n",
       " 'rjo': 169,\n",
       " 'ro': 170,\n",
       " 'ron': 171,\n",
       " 'rra': 172,\n",
       " 'rre': 173,\n",
       " 'rro': 174,\n",
       " 's': 175,\n",
       " 's@': 176,\n",
       " 'sE': 177,\n",
       " 'sEt': 178,\n",
       " 'sO': 179,\n",
       " 'sa': 180,\n",
       " 'sa~': 181,\n",
       " 'se': 182,\n",
       " 'si': 183,\n",
       " 'sjo~': 184,\n",
       " 'so': 185,\n",
       " 'so~': 186,\n",
       " 'spe': 187,\n",
       " 'sse': 188,\n",
       " 'ssi': 189,\n",
       " 'sso': 190,\n",
       " 'sta': 191,\n",
       " 'ste': 192,\n",
       " 'sti': 193,\n",
       " 'sto': 194,\n",
       " 'su': 195,\n",
       " 'syR': 196,\n",
       " 't': 197,\n",
       " 't@': 198,\n",
       " 'tE': 199,\n",
       " 'tS': 200,\n",
       " 'tSi': 201,\n",
       " 'ta': 202,\n",
       " 'tan': 203,\n",
       " 'ta~': 204,\n",
       " 'te': 205,\n",
       " 'ter': 206,\n",
       " 'tes': 207,\n",
       " 'ti': 208,\n",
       " 'to': 209,\n",
       " 'tra': 210,\n",
       " 'tre': 211,\n",
       " 'tro': 212,\n",
       " 'ts': 213,\n",
       " 'tsa': 214,\n",
       " 'ttS': 215,\n",
       " 'tta': 216,\n",
       " 'tte': 217,\n",
       " 'tti': 218,\n",
       " 'tto': 219,\n",
       " 'tts': 220,\n",
       " 'ttsjo': 221,\n",
       " 'tu': 222,\n",
       " 'u': 223,\n",
       " 'un': 224,\n",
       " 'v': 225,\n",
       " 'vE': 226,\n",
       " 'va': 227,\n",
       " 've': 228,\n",
       " 'vi': 229,\n",
       " 'vo': 230,\n",
       " 'vu': 231,\n",
       " 'w': 232,\n",
       " 'x': 233,\n",
       " 'xe': 234,\n",
       " 'xi': 235,\n",
       " 'y': 236,\n",
       " 'yn': 237,\n",
       " 'z': 238,\n",
       " 'za': 239,\n",
       " 'ze': 240,\n",
       " 'zi': 241,\n",
       " '~': 242,\n",
       " '|': 243,\n",
       " '[UNK]': 244,\n",
       " '[PAD]': 245}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLphoClasses = {'e': 0, 'i': 0, 'o': 0,'u': 0, 'a': 0, '@': 0, 'E':0, 'O':0, 'y':0, '2':0, '9':0, # vowels\n",
    "                 'A':0, 'e~':0, 'a~':0, 'o~':0, '9~':0, '~': 11,\n",
    "                 'j': 1, 'w': 1, 'H':1,\n",
    "                 'r': 2, 'R': 2,\n",
    "                 'l': 3,'L': 3,\n",
    "                 'm': 4, 'n': 4, 'J': 4, 'N': 4,\n",
    "                 's': 5, 'z': 5,\n",
    "                 'v': 6, 'f': 6,\n",
    "                 'B': 7, 'D': 7, 'G': 7, 'Z': 7, 'S': 7, 'T': 7, 'x': 7, 'X': 7,\n",
    "                 'ddz': 8, 'dz': 8, 'ts': 8, 'tts': 8, 'ddZ' : 8, 'dZ' : 8, 'tS' : 8, 'ttS' : 8,\n",
    "                 'b': 9, 'd': 9, 'g': 9, 'k': 9, 'p': 9,'t': 9}\n",
    "\n",
    "\n",
    "segmML_list = [k for k in MLphoClasses.keys()] # single phonemes\n",
    "\n",
    "sylMLT_list = MLmostFsyl\n",
    "\n",
    "MLTsylphoVocab = segmML_list + sylMLT_list\n",
    "print(len(MLTsylphoVocab))\n",
    "\n",
    "setlist = list(set(MLTsylphoVocab))\n",
    "print(len(setlist))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(setlist))}\n",
    "vocab_dict\n",
    "\n",
    "\n",
    "# Let's add \" \" as a more visible character, |.\n",
    "vocab_dict[\"|\"] = len(vocab_dict)\n",
    "\n",
    "# # Then, we  add an \"unknown\" token so that the model can later deal with characters not encountered in CV's training set.\n",
    "# # We also add the unknown label and the pad token (CTC blank token) to the dictionary.\n",
    "\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "vocab_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vocabulary in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'ML'\n",
    "units = 'PhoSyl'\n",
    "u_count = len(vocab_dict)\n",
    "# saving it as a json file in the current directory\n",
    "vocab_name = f'vocab{lang}_{units}{u_count}.json'\n",
    "with open(vocab_name, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save the tokenizer folder based on the vocabulary we just created and on the custom tokenizer that works according to the syllabification rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomML_ITESFRPhoSylCTCTokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizerML_hybPhoSyl246\\\\tokenizer_config.json',\n",
       " './tokenizerML_hybPhoSyl246\\\\special_tokens_map.json',\n",
       " './tokenizerML_hybPhoSyl246\\\\vocab.json',\n",
       " './tokenizerML_hybPhoSyl246\\\\added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = HybridML_ITESFRPhoSylCTCTokenizer(f'./vocab{lang}_{units}{u_count}.json', unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" | \")\n",
    "tokenizer.save_pretrained(f\"./tokenizer{lang}_hyb{units}{u_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cb8d6a7a1ec48364c58efa374d1a4f5aba50f1a80848d19fa6d9ee12721c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
