{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will:\n",
    "\n",
    "* extract data from the [Mozilla Common Voice](https://commonvoice.mozilla.org/it?gclid=Cj0KCQiA2-2eBhClARIsAGLQ2RlkVJtTFkEemoK3FvlpTxtFwuXvAHGOHadvXjzcbrx-R2Jw9eNdES8aAhcPEALw_wcB) corpus through the Hugging Face Hub\n",
    "* build a multilingual corpus\n",
    "* transcribe the corpus through the [WebMAUS Basic](https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSBasic) tool\n",
    "\n",
    "\n",
    "Our corpus will consist of approximately 20 hours of speech data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install datasets==2.1\n",
    "!pip install transformers==4.18 \n",
    "!pip install huggingface_hub==0.5.1 \n",
    "!pip install torchaudio==0.11  \n",
    "!pip install librosa \n",
    "!pip install jiwer   \n",
    "!git config --global credential.helper store \n",
    "!apt install git-lfs\n",
    "!%pip install sox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from IPython.display import display, HTML\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer, AutoModelForCTC, Wav2Vec2Processor\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from datasets.utils.version import Version\n",
    "from datasets import load_dataset, load_metric, Audio\n",
    "from torch import Tensor\n",
    "import sys\n",
    "import argparse\n",
    "import requests\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Common Voice datasets\n",
    "\n",
    "The corpus will be composed of Italian, Spanish and French data; if you want to work with other languages change the language id according to the data that you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing functions\n",
    "\n",
    "def computeTotLen(dataset):\n",
    "\n",
    "  \"\"\"computes length of the corpus in seconds\"\"\"\n",
    "\n",
    "  len_audio_fs = [ ]\n",
    "  for el in dataset['audio']:\n",
    "    len_audio = len(el['array'])/el['sampling_rate']\n",
    "    len_audio_fs.append(len_audio)\n",
    "  tot = sum(list(len_audio_fs))\n",
    "  # print(len_audio_fs)\n",
    "  return tot\n",
    "\n",
    "# functions to clean the text \n",
    "\n",
    "chars_to_remove_regex = '[\\,\\#\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\°\\(\\)\\–\\…\\\\\\[\\]\\«\\»\\\\\\/\\^\\<\\>\\~\\_\\-\\¿\\¡\\—]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "    return batch\n",
    "\n",
    "def replace_hatted_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub('[’]', \"'\", batch[\"sentence\"])\n",
    "    return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italian data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*------- working on IT dataset -------*\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StreamingDownloadManager' object has no attribute 'is_streaming'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zac18\\OneDrive\\Documenti\\GitHub\\get_transcribe_data.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m*------- working on IT dataset -------*\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# let's load the dataset and get rid of some columns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m common_voice_trainIT \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mmozilla-foundation/common_voice_11_0\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mit\u001b[39;49m\u001b[39m'\u001b[39;49m , split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain[:50]\u001b[39;49m\u001b[39m\"\u001b[39;49m, streaming\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# common_voice_trainIT = load_dataset(\"common_voice\", 'it' , split=\"train[:7000]\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m common_voice_trainIT \u001b[39m=\u001b[39m common_voice_trainIT\u001b[39m.\u001b[39mremove_columns([\u001b[39m\"\u001b[39m\u001b[39maccent\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclient_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdown_votes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlocale\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zac18/OneDrive/Documenti/GitHub/get_transcribe_data.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39msegment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mup_votes\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\zac18\\.conda\\envs\\tesi\\lib\\site-packages\\datasets\\load.py:1692\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[0;32m   1690\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n\u001b[0;32m   1691\u001b[0m     extend_dataset_builder_for_streaming(builder_instance, use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[1;32m-> 1692\u001b[0m     \u001b[39mreturn\u001b[39;00m builder_instance\u001b[39m.\u001b[39;49mas_streaming_dataset(\n\u001b[0;32m   1693\u001b[0m         split\u001b[39m=\u001b[39;49msplit,\n\u001b[0;32m   1694\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1695\u001b[0m     )\n\u001b[0;32m   1697\u001b[0m \u001b[39m# Some datasets are already processed on the HF google storage\u001b[39;00m\n\u001b[0;32m   1698\u001b[0m \u001b[39m# Don't try downloading from google storage for the packaged datasets as text, json, csv or pandas\u001b[39;00m\n\u001b[0;32m   1699\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n",
      "File \u001b[1;32mc:\\Users\\zac18\\.conda\\envs\\tesi\\lib\\site-packages\\datasets\\builder.py:895\u001b[0m, in \u001b[0;36mDatasetBuilder.as_streaming_dataset\u001b[1;34m(self, split, base_path, use_auth_token)\u001b[0m\n\u001b[0;32m    888\u001b[0m dl_manager \u001b[39m=\u001b[39m StreamingDownloadManager(\n\u001b[0;32m    889\u001b[0m     base_path\u001b[39m=\u001b[39mbase_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_path,\n\u001b[0;32m    890\u001b[0m     download_config\u001b[39m=\u001b[39mDownloadConfig(use_auth_token\u001b[39m=\u001b[39muse_auth_token),\n\u001b[0;32m    891\u001b[0m     dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[0;32m    892\u001b[0m     data_dir\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_dir,\n\u001b[0;32m    893\u001b[0m )\n\u001b[0;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_manual_download(dl_manager)\n\u001b[1;32m--> 895\u001b[0m splits_generators \u001b[39m=\u001b[39m {sg\u001b[39m.\u001b[39mname: sg \u001b[39mfor\u001b[39;00m sg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager)}\n\u001b[0;32m    896\u001b[0m \u001b[39m# By default, return all splits\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\mozilla-foundation--common_voice_11_0\\2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0\\common_voice_11_0.py:145\u001b[0m, in \u001b[0;36mCommonVoice._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    141\u001b[0m     audio_urls[split] \u001b[39m=\u001b[39m [\n\u001b[0;32m    142\u001b[0m         _AUDIO_URL\u001b[39m.\u001b[39mformat(lang\u001b[39m=\u001b[39mlang, split\u001b[39m=\u001b[39msplit, shard_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_shards[lang][split])\n\u001b[0;32m    143\u001b[0m     ]\n\u001b[0;32m    144\u001b[0m archive_paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownload(audio_urls)\n\u001b[1;32m--> 145\u001b[0m local_extracted_archive_paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mextract(archive_paths) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dl_manager\u001b[39m.\u001b[39;49mis_streaming \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    147\u001b[0m meta_urls \u001b[39m=\u001b[39m {split: _TRANSCRIPT_URL\u001b[39m.\u001b[39mformat(lang\u001b[39m=\u001b[39mlang, split\u001b[39m=\u001b[39msplit) \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m splits}\n\u001b[0;32m    148\u001b[0m meta_paths \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownload_and_extract(meta_urls)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StreamingDownloadManager' object has no attribute 'is_streaming'"
     ]
    }
   ],
   "source": [
    "print('*------- working on IT dataset -------*')\n",
    "\n",
    "# let's load the dataset and get rid of some columns\n",
    "\n",
    "common_voice_trainIT = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'it' , split=\"train[:7000]\")\n",
    "common_voice_trainIT = common_voice_trainIT.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_testIT = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'it', split=\"test[:1400]\")\n",
    "common_voice_testIT = common_voice_testIT.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_validationIT = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'it', split=\"validation[:1400]\")\n",
    "common_voice_validationIT = common_voice_validationIT.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check corpus length\n",
    "\n",
    "print('dataset initial len: ',len(common_voice_trainIT), len(common_voice_testIT), len(common_voice_validationIT))\n",
    "\n",
    "ITlen_pre_filterTR = computeTotLen(common_voice_trainIT)\n",
    "ITlen_pre_filterTST = computeTotLen(common_voice_testIT)\n",
    "ITlen_pre_filterVAL = computeTotLen(common_voice_validationIT)\n",
    "\n",
    "print(f'IT dataset len in sec pre filter TRAIN: {ITlen_pre_filterTR} - TEST: {ITlen_pre_filterTST} - VAL {ITlen_pre_filterVAL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*------- cleaning text -------*')\n",
    "\n",
    "# remove special chrs\n",
    "common_voice_trainIT = common_voice_trainIT.map(remove_special_characters)\n",
    "common_voice_testIT = common_voice_testIT.map(remove_special_characters)\n",
    "common_voice_validationIT = common_voice_validationIT.map(remove_special_characters)\n",
    "\n",
    "\n",
    "# replace hatted chrs\n",
    "common_voice_trainIT = common_voice_trainIT.map(replace_hatted_characters)\n",
    "common_voice_testIT = common_voice_testIT.map(replace_hatted_characters)\n",
    "common_voice_validationIT = common_voice_validationIT.map(replace_hatted_characters)\n",
    "\n",
    "print('----------cleaning text done-------*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract files that are max 7 seconds long\n",
    "\n",
    "print('*------- filtering by length -------*')\n",
    "\n",
    "max_input_length_in_sec = 7.0\n",
    "\n",
    "common_voice_trainIT = common_voice_trainIT.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "common_voice_testIT = common_voice_testIT.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "common_voice_validationIT = common_voice_validationIT.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "\n",
    "print(f'number of files: TRAIN: {len(common_voice_trainIT)}, TEST:  {len(common_voice_testIT)}, VAL: {len(common_voice_validationIT)}')\n",
    "\n",
    "\n",
    "ITlen_post_filterTR = computeTotLen(common_voice_trainIT)\n",
    "ITlen_post_filterTST = computeTotLen(common_voice_testIT)\n",
    "ITlen_post_filterVAL = computeTotLen(common_voice_validationIT)\n",
    "\n",
    "print(f'IT dataset len in sec post filter TRAIN: {ITlen_post_filterTR} - TEST: {ITlen_post_filterTST} - VAL {ITlen_post_filterVAL}')\n",
    "\n",
    "\n",
    "print('*-------- audio filtering done --------*')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*------- working on ES dataset -------*')\n",
    "\n",
    "common_voice_trainES = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'es' , split=\"train[:7000]\")\n",
    "common_voice_trainES = common_voice_trainES.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\",\n",
    "\"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_testES = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'es' , split=\"test[:1400]\")\n",
    "common_voice_testES = common_voice_testES.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_validationES = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'es', split=\"validation[:1400]\")\n",
    "common_voice_validationES = common_voice_validationES.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\",\n",
    "\"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "print(len(common_voice_trainES), len(common_voice_testES), len(common_voice_validationES))\n",
    "\n",
    "ESlen_pre_filterTR = computeTotLen(common_voice_trainES)\n",
    "ESlen_pre_filterTST = computeTotLen(common_voice_testES)\n",
    "ESlen_pre_filterVAL = computeTotLen(common_voice_validationES)\n",
    "\n",
    "print(f'ES dataset len in sec pre filter TRAIN: {ESlen_pre_filterTR} - TEST: {ESlen_pre_filterTST} - VAL {ESlen_pre_filterVAL}')\n",
    "\n",
    "\n",
    "print('*------- cleaning text -------*')\n",
    "\n",
    "# remove special chrs\n",
    "common_voice_trainES = common_voice_trainES.map(remove_special_characters)\n",
    "common_voice_testES = common_voice_testES.map(remove_special_characters)\n",
    "common_voice_validationES = common_voice_validationES.map(remove_special_characters)\n",
    "\n",
    "\n",
    "# replace hatted chrs\n",
    "common_voice_trainES = common_voice_trainES.map(replace_hatted_characters)\n",
    "common_voice_testES = common_voice_testES.map(replace_hatted_characters)\n",
    "common_voice_validationES = common_voice_validationES.map(replace_hatted_characters)\n",
    "\n",
    "print('----------cleaning text done-------*')\n",
    "\n",
    "\n",
    "## --------------- AUDIO LEN FILTER\n",
    "\n",
    "print('*------- filtering by length -------*')\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "common_voice_trainES = common_voice_trainES.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "# TEST\n",
    "\n",
    "common_voice_testES = common_voice_testES.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "# # VAL\n",
    "\n",
    "common_voice_validationES = common_voice_validationES.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "\n",
    "print(f'number of files: TRAIN: {len(common_voice_trainES)}, TEST:  {len(common_voice_testES)}, VAL: {len(common_voice_validationES)}')\n",
    "\n",
    "\n",
    "ESlen_post_filterTR = computeTotLen(common_voice_trainES)\n",
    "ESlen_post_filterTST = computeTotLen(common_voice_testES)\n",
    "ESlen_post_filterVAL = computeTotLen(common_voice_validationES)\n",
    "\n",
    "print(f'ES dataset len in sec post filter TRAIN: {ESlen_post_filterTR} - TEST: {ESlen_post_filterTST} - VAL {ESlen_post_filterVAL}')\n",
    "\n",
    "\n",
    "print('*-------- audio filtering done --------*')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*------- working on FR dataset -------*')\n",
    "\n",
    "common_voice_trainFR = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'fr' , split=\"train[:7000]\")\n",
    "common_voice_trainFR = common_voice_trainFR.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\",\n",
    "\"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_testFR = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'fr', split=\"test[:1400]\")\n",
    "common_voice_testFR = common_voice_testFR.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "common_voice_validationFR = load_dataset(\"mozilla-foundation/common_voice_11_0\", 'fr', split=\"validation[:1400]\")\n",
    "common_voice_validationFR = common_voice_validationFR.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\",\n",
    "\"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "\n",
    "print(len(common_voice_trainFR), len(common_voice_testFR), len(common_voice_validationFR))\n",
    "\n",
    "FRlen_pre_filterTR = computeTotLen(common_voice_trainFR)\n",
    "FRlen_pre_filterTST = computeTotLen(common_voice_testFR)\n",
    "FRlen_pre_filterVAL = computeTotLen(common_voice_validationFR)\n",
    "\n",
    "print(f'FR dataset len in sec pre filter TRAIN: {FRlen_pre_filterTR} - TEST: {FRlen_pre_filterTST} - VAL {FRlen_pre_filterVAL}')\n",
    "\n",
    "\n",
    "print('*------- cleaning text -------*')\n",
    "\n",
    "# remove special chrs\n",
    "common_voice_trainFR = common_voice_trainFR.map(remove_special_characters)\n",
    "common_voice_testFR = common_voice_testFR.map(remove_special_characters)\n",
    "common_voice_validationFR = common_voice_validationFR.map(remove_special_characters)\n",
    "\n",
    "\n",
    "# replace hatted chrs\n",
    "common_voice_trainFR = common_voice_trainFR.map(replace_hatted_characters)\n",
    "common_voice_testFR = common_voice_testFR.map(replace_hatted_characters)\n",
    "common_voice_validationFR = common_voice_validationFR.map(replace_hatted_characters)\n",
    "\n",
    "print('----------cleaning text done-------*')\n",
    "\n",
    "\n",
    "## --------------- AUDIO LEN FILTER\n",
    "\n",
    "print('*------- filtering by length -------*')\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "common_voice_trainFR = common_voice_trainFR.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "# TEST\n",
    "\n",
    "common_voice_testFR = common_voice_testFR.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "# # VAL\n",
    "\n",
    "common_voice_validationFR = common_voice_validationFR.filter(lambda x: len(x['audio']['array'])/x['audio']['sampling_rate'] < max_input_length_in_sec, num_proc=5)\n",
    "\n",
    "\n",
    "print(f'number of files: TRAIN: {len(common_voice_trainFR)}, TEST:  {len(common_voice_testFR)}, VAL: {len(common_voice_validationFR)}')\n",
    "\n",
    "\n",
    "FRlen_post_filterTR = computeTotLen(common_voice_trainFR)\n",
    "FRlen_post_filterTST = computeTotLen(common_voice_testFR)\n",
    "FRlen_post_filterVAL = computeTotLen(common_voice_validationFR)\n",
    "\n",
    "print(f'FR dataset len in sec post filter TRAIN: {FRlen_post_filterTR} - TEST: {FRlen_post_filterTST} - VAL {FRlen_post_filterVAL}')\n",
    "\n",
    "\n",
    "print('*-------- audio filtering done --------*')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic phonological transcriptions - WebMAUS Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(audio_dataset):\n",
    "\n",
    "    \"\"\"Returns a list of dictionaries in which filename, length and text of each file in the corpus are stored\"\"\"\n",
    "    \n",
    "    list_ds = [ ]\n",
    "\n",
    "    for el in audio_dataset:    \n",
    "        audio_info = {'filename': el['audio']['path'], 'length': len(el['audio']['array'])/el['audio']['sampling_rate'], 'text': el['sentence']}\n",
    "        list_ds.append(audio_info)\n",
    "\n",
    "    return list_ds\n",
    "\n",
    "\n",
    "def remove_path(list_ds):\n",
    "\n",
    "    \"\"\"Removes path from filename\"\"\"\n",
    "    for audio in list_ds:\n",
    "        audio['filename'] = os.path.split(audio['filename'])[-1]\n",
    "    return list_ds  \n",
    "\n",
    "\n",
    "def filename_list(info_list):\n",
    "    filenames_l = [ ]\n",
    "    for el in info_list:\n",
    "        filenames_l.append(el['filename'])\n",
    "    print(len(filenames_l))\n",
    "    return filenames_l\n",
    "\n",
    "\n",
    "def sent2textf(info_list):\n",
    "\n",
    "    \"\"\"stores the text of each file of the corpus in current directory\"\"\"\n",
    "\n",
    "    for audio_f in info_list:\n",
    "        filename = re.sub('.mp3', '', audio_f['filename'])\n",
    "    # print(type(filename))\n",
    "    # print(filename)\n",
    "        content = audio_f['text']\n",
    "        with open(filename+'.txt', 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "\n",
    "def webMAUS_processing(txt_path, out_format, out_path, info_dict, language = \"ita-IT\"):\n",
    "\n",
    "    \"\"\"takes audio and text file from and returns a csv with phonological and phonetic transcription\"\"\"\n",
    "\n",
    "    url = \"https://clarin.phonetik.uni-muenchen.de/BASWebServices/services/runMAUSBasic\"\n",
    "    counter = 0\n",
    "    for filen in info_dict:\n",
    "        filename = os.path.splitext(filen['filename'])[0]\n",
    "        print(filename)\n",
    "        textfile = txt_path+filename+'.txt'\n",
    "        print(textfile)\n",
    "        mp3file = glob.glob(os.path.dirname(filen['path'])+'/*/')[0]+filename+'.mp3'\n",
    "        print(mp3file)\n",
    "        formdata = {\n",
    "            \"SIGNAL\": (os.path.split(mp3file)[-1], open(mp3file, \"rb\"), \"audio/x-mp3\"),\n",
    "            \"TEXT\": (os.path.split(textfile)[-1], open(textfile, \"r\"), \"text/txt\"),\n",
    "            \n",
    "            \"LANGUAGE\": (None, language),\n",
    "            \"OUTFORMAT\": (None, out_format)\n",
    "        }\n",
    "\n",
    "        res = requests.post(url, files=formdata)\n",
    "\n",
    "        lista_out = [ ]\n",
    "        lista_out.append(res.text)\n",
    "\n",
    "    # Driver Code\n",
    "\n",
    "        for str_obj in lista_out:\n",
    "            string = str_obj\n",
    "            link = Find(string) # returns link in a list\n",
    "\n",
    "        link = link[0] # flat\n",
    "\n",
    "        r = requests.get(bbb, allow_redirects=True)\n",
    "        open(out_path+filename+'.'+out_format, 'wb').write(r.content) \n",
    "        counter +=1\n",
    "    print('processed files: ', counter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) store the information about each file in a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_common_voice_trainIT = filter_length(common_voice_trainIT) \n",
    "info_common_voice_trainIT = remove_path(info_common_voice_trainIT) \n",
    "\n",
    "info_common_voice_testIT = filter_length(common_voice_testIT)\n",
    "info_common_voice_testIT = remove_path(info_common_voice_testIT)\n",
    "\n",
    "info_common_voice_validationIT = filter_length(common_voice_validationIT)\n",
    "info_common_voice_validationIT = remove_path(info_common_voice_validationIT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) create and set a directory to store the .txt files corresponing to the transcriptions. <br/>\n",
    "It is convenient to store each subset in different directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /my_txt_directory_train\n",
    "!mkdir /my_txt_directory_test\n",
    "!mkdir /my_txt_directory_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) store the sentences of the corpus in separate .txt files and save them in a directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd my_new_directory_train\n",
    "sent2textf(info_common_voice_trainIT)\n",
    "\n",
    "!pwd my_new_directory_test\n",
    "sent2textf(info_common_voice_testIT)\n",
    "\n",
    "!pwd my_new_directory_val\n",
    "sent2textf(info_common_voice_validationIT)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Call the WebMAUS_processing function for each to store the csv with the transcriptions corresponding to each file of the corpus in the out_directory.\n",
    "\n",
    "Note: this process can take a while. If you're working with a large amount of files if could be convenient to download the audio and the text files on your local machine and generate the csv directly form the WebMAUS Basic interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webMAUS_processing('/my_txt_directory_train/', 'csv', '/my_csv_directory_train/', info_common_voice_trainIT)\n",
    "webMAUS_processing('/my_txt_directory_test/', 'csv', '/my_csv_directory_test/', info_common_voice_testIT)\n",
    "webMAUS_processing('/my_txt_directory_val/', 'csv', '/my_csv_directory_val/', info_common_voice_validationIT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phonological transcription is just the set of a list \n",
    "\n",
    "def clean_transcr(seq):\n",
    "\n",
    "    \"\"\"Removes duplicates from a list while preserving order - since the phonological trascription\n",
    "    in MAUS CSV is the same word repeated in several cells representing the duration of the word\n",
    "    in time\"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    transcr = [x for x in seq if not (x in seen or seen_add(x))]\n",
    "    return transcr\n",
    "\n",
    "\n",
    "\n",
    "def get_transcriptions():\n",
    "\n",
    "\n",
    "    \"\"\"Process all the CSV files in the cwd to extract the phonological and phonetic transcription\"\"\"\n",
    "\n",
    "    # make sure to be in the dir where the csv files are stored\n",
    "    files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "    print(len(files))\n",
    "   \n",
    "    df_list = [ ]\n",
    "    for csv in files: \n",
    "        dd = { 'filename' : None, 'phonl_tr' : None, 'phont_tr' : None }\n",
    "        df = pd.read_csv(csv, sep=';')\n",
    "\n",
    "        # phonological transcription\n",
    "        filename = os.path.splitext(csv)[0]\n",
    "        phonol_temp = [ ]\n",
    "        phonol_sent = [ ]\n",
    "        for word in df['KAN']:\n",
    "            if isinstance(word, str) == True:\n",
    "                no_space = \"\".join(word.split())\n",
    "                phonol_temp.append(no_space)     \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            clean_phonol_s = clean_transcr(phonol_temp)\n",
    "            phonol_sent = ' '.join(clean_phonol_s)\n",
    "     \n",
    "        # phonetic transcription \n",
    "        phon_temp = [ ]\n",
    "        word = \"\"\n",
    "        current_token = None\n",
    "        for index, row in df.iterrows():   \n",
    "            if row['TOKEN'] != -1:\n",
    "                # Update current token and reset word\n",
    "                if row['TOKEN'] != current_token:\n",
    "                    if word != \"\":\n",
    "                        phon_temp.append(word)\n",
    "                    current_token = row['TOKEN']\n",
    "                    word = \"\"\n",
    "                # Append to form the word\n",
    "                word = word + row['MAU']\n",
    "\n",
    "                if index == len(df)-1:  \n",
    "                    phon_temp.append(word)\n",
    "\n",
    "            else:\n",
    "                if index == len(df)-1:  \n",
    "                    phon_temp.append(word) \n",
    "                    \n",
    "            phon_sent = ' '.join(phon_temp)\n",
    "        \n",
    "        dd['filename'] = filename\n",
    "        dd['phonl_tr'] = phonol_sent\n",
    "        dd['phont_tr'] = phon_sent\n",
    "\n",
    "        # print('phonological: ', phonol_sent, 'phonetic: ', phon_sent)\n",
    "        df_list.append(dd)\n",
    "       \n",
    "    return df_list\n",
    "\n",
    "\n",
    "def transcriptions_df(setname, out_dir):\n",
    "    transcr_list = get_transcriptions()\n",
    "    transcr_df =  pd.DataFrame(transcr_list)\n",
    "    with open(f'{out_dir}{setname}df_transcriptions.pkl', \"wb\") as fp:   #Pickling\n",
    "    pkl.dump(transcr_df, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_df('it_train')\n",
    "transcriptions_df('it_test')\n",
    "transcriptions_df('it_val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcriptions cleaning\n",
    "\n",
    "Acronyms, abbreviations, any non-alphabetic character and non-spelled numbers are problematic elements because the graphemic correspondence do not match equally with what is actually pronounced. Foreign words, proper nouns, toponyms, and onomatopoeias are also a source of issues: they may be composed by an unusual grapheme sequence for a certain language and, in addition, some graphemes may not have a correspondent in the phoneme inventory that is used to define the transcription rules implemented in the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcrCleanerMltLng(string, lang = 'es'):\n",
    "\n",
    "  \"\"\"takes in a string, gets rid of the spaces inbetween words and of non A-Z characters,\n",
    "  puts a space in between each character and returns the string \"\"\"\n",
    "\n",
    "  \n",
    "  chars_to_remove_regex = r'<usb>'\n",
    "  \n",
    "  no_weird1 = re.sub(chars_to_remove_regex, '', string)\n",
    "\n",
    "  chars_to_remove_regex = r'<p:>'\n",
    "  no_weird2 = re.sub(chars_to_remove_regex, '', no_weird1)\n",
    "\n",
    "  chars_to_remove_regex = '_'\n",
    "  no_weird3 = re.sub(chars_to_remove_regex, ' ', no_weird2)\n",
    "\n",
    "  chars_to_remove_regex = '[#_<>]'\n",
    "  no_weird4 = re.sub(chars_to_remove_regex, '', no_weird3)\n",
    "  if '?' in no_weird4:\n",
    "     no_weird4 = no_weird4.replace('?', '')\n",
    "  \n",
    "  if 'q' in no_weird4:\n",
    "     no_weird4 = no_weird4.replace('q', 'k')\n",
    "\n",
    "\n",
    "  if 'ù' in no_weird4:\n",
    "     no_weird4 = no_weird4.replace('ù', 'u')\n",
    "\n",
    "  if lang == 'it':\n",
    "    if 'ch' in no_weird4:\n",
    "        no_weird4 = no_weird4.replace('ch', 'k')\n",
    "\n",
    "    no_weird4 = re.sub(r'[^a-zA-Z]+', ' ', no_weird4).strip()\n",
    "\n",
    "  elif lang == 'fr':\n",
    "    if 'r' in no_weird4:\n",
    "      print('r found')\n",
    "      no_weird4 = no_weird4.replace('r', 'R')\n",
    "      \n",
    "    no_weird4 = re.sub(r'[^a-zA-Z29@~]+', ' ', no_weird4).strip()\n",
    "\n",
    "  return no_weird4\n",
    "\n",
    "\n",
    "def find_acronyms(batch):\n",
    "\n",
    "  \"\"\"finds words without vowels in transcriptions. Returns a list with the\n",
    "  index of the example in the corpus with the problematic transcription so that is possible\n",
    "  to retrieve it and check the context\"\"\"\n",
    "\n",
    "  all_prob_w = [ ]\n",
    "  for i, sent in enumerate(batch['phonl_tr']):\n",
    "    x = sent.split()\n",
    "    # print(x)\n",
    "    prob_words = [word for word in x if len(word) > 1 and not any(v in word for v in nucleus_scw)] #and len(word) >1]\n",
    "    if prob_words != [ ]:\n",
    "      all_prob_w.append([i, prob_words])\n",
    "    else:\n",
    "      pass\n",
    "  \n",
    "  return all_prob_w\n",
    "\n",
    "def most_frequent(List):\n",
    "\n",
    "    \"\"\"finds most frequent acronym/abbreviation\"\"\"\n",
    "    \n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def affricates(str_chrs):\n",
    "  if \"d d z\" in str_chrs:\n",
    "    x = str_chrs.replace(\"d d z\", 'ddz')\n",
    "  else:\n",
    "    x = str_chrs\n",
    "  if \"d d Z\"  in x:\n",
    "    x = x.replace(\"d d Z\",  'ddZ')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"d z\"  in x:\n",
    "    x = x.replace(\"d z\",  'dz')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"d Z\"  in x:\n",
    "    x = x.replace(\"d Z\",  'dZ')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t t s\"  in x:\n",
    "    x = x.replace(\"t t s\",  'tts')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t t S\"  in x:\n",
    "    x = x.replace(\"t t S\",  'ttS')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  if \"t s\"  in x:\n",
    "    x = x.replace(\"t s\",  'ts')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t S\"  in x:\n",
    "    x = x.replace(\"t S\",  'tS')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  return x\n",
    "\n",
    "def affricates_nasV(str_chrs):\n",
    "  if \"d d z\" in str_chrs:\n",
    "    x = str_chrs.replace(\"d d z\", 'ddz')\n",
    "  else:\n",
    "    x = str_chrs\n",
    "  if \"d d Z\"  in x:\n",
    "    x = x.replace(\"d d Z\",  'ddZ')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"d z\"  in x:\n",
    "    x = x.replace(\"d z\",  'dz')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"d Z\"  in x:\n",
    "    x = x.replace(\"d Z\",  'dZ')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t t s\"  in x:\n",
    "    x = x.replace(\"t t s\",  'tts')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t t S\"  in x:\n",
    "    x = x.replace(\"t t S\",  'ttS')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  if \"t s\"  in x:\n",
    "    x = x.replace(\"t s\",  'ts')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"t S\"  in x:\n",
    "    x = x.replace(\"t S\",  'tS')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  # nasals\n",
    "\n",
    "  if \"e ~\"  in x:\n",
    "    x = x.replace(\"e ~\",  'e~')\n",
    "  else:\n",
    "    x = x \n",
    "  if \"a ~\"  in x:\n",
    "    x = x.replace(\"a ~\",  'a~')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  if \"o ~\"  in x:\n",
    "    x = x.replace(\"o ~\",  'o~')\n",
    "  else:\n",
    "    x = x \n",
    "\n",
    "  if \"9 ~\"  in x:\n",
    "    x = x.replace(\"9 ~\",  '9~')\n",
    "  else:\n",
    "    x = x  \n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def fix_transcriptions(sent, acr, abbr, oth):\n",
    "\n",
    "  \"\"\"Takes a string and replaces the wrong transcriptions relying on a\n",
    "  language-dependent spelling dictionary, abbreviation dictionary and \n",
    "  custom dictionary with inconsistencies\"\"\"\n",
    "\n",
    "  fixed_sent = [ ]\n",
    "  # for i, sent in enumerate(batch['phonl_tr']):\n",
    "  #   x = sent.split()\n",
    "  s_list = sent.split()\n",
    "\n",
    "  for word in s_list:\n",
    "    if word in oth:\n",
    "      fixed_tr = re.sub(word, oth[word], word)\n",
    "      fixed_sent.append(fixed_tr)\n",
    "    elif word in abbr:\n",
    "      fixed_tr = re.sub(word, abbr[word], word)\n",
    "      fixed_sent.append(fixed_tr)\n",
    "    elif len(word) > 1 and not any(v in word for v in nucleus_scw):\n",
    "      expl_acr = [ ]\n",
    "      word_chrs = \" \".join(word)\n",
    "      word_chrs = affricates_nasV(word_chrs)\n",
    "      # print(word_chrs)\n",
    "      w_c_list = word_chrs.split()\n",
    "      # print(w_c_list)\n",
    "      for chr in w_c_list:\n",
    "        # print(chr)    \n",
    "        fixed_ch = re.sub(chr, acr[chr], chr)\n",
    "        expl_acr.append(fixed_ch)\n",
    "      \n",
    "      # print(expl_acr)\n",
    "      fixed_tr = ''.join(expl_acr)\n",
    "      fixed_sent.append(fixed_tr)\n",
    "\n",
    "\n",
    "    else:\n",
    "      fixed_tr = word\n",
    "      fixed_sent.append(fixed_tr)\n",
    "    \n",
    "  fixed_sent = ' '.join(fixed_sent) \n",
    "  \n",
    "  \n",
    "  return fixed_sent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning IT transcriptions\n",
    "\n",
    "# train\n",
    "IT_train_df['phonl_tr'] = IT_train_df['phonl_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "IT_train_df['phont_tr'] = IT_train_df['phont_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "\n",
    "#test\n",
    "IT_test_df['phonl_tr'] = IT_test_df['phonl_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "IT_test_df['phont_tr'] = IT_test_df['phont_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "\n",
    "#val\n",
    "IT_val_df['phont_tr'] = IT_val_df['phont_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "IT_val_df['phonl_tr'] = IT_val_df['phonl_tr'].apply(transcrCleanerMltLng, args=['it'])\n",
    "\n",
    "# finding problematic word transcriptions\n",
    "\n",
    "ITacr_train = find_acronyms(IT_train_df) #219\n",
    "ITacr_test = find_acronyms(IT_test_df) #55\n",
    "ITacr_val = find_acronyms(IT_val_df) #50\n",
    "\n",
    "# set of italian problematic transcriptions\n",
    "\n",
    "all_probs_s = list(set(all_probs_flat)) # 89 items\n",
    "all_probs_s\n",
    "\n",
    "# checking for a specific transcription on the corpus by index\n",
    "\n",
    "print(IT_train_df['phonl_tr'][14])\n",
    "print(IT_train_df['filename'][22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language-dependent vocabularies to fix the issues\n",
    "\n",
    "IT_spelling = {'b' : 'bi', \n",
    "               'tS' : 'tSi', \n",
    "               'ttS' : 'tSi',\n",
    "               'd' : 'di',\n",
    "               'f' : 'effe',\n",
    "               'dZ' : 'dZi',\n",
    "               'dz' : 'dzi',\n",
    "               'g' : 'dZi', \n",
    "               'h' : 'akka', \n",
    "               'j' : 'dZei',\n",
    "               'k' : 'kappa',\n",
    "               'l' : 'elle',\n",
    "               'm' : 'emme',\n",
    "               'n' : 'enne',\n",
    "               'p' : 'pi',\n",
    "               'q' : 'ku',\n",
    "               'r' : 'erre',\n",
    "               's' : 'esse',\n",
    "               't' : 'ti',\n",
    "               'v' : 'vu',\n",
    "               'x' : 'iks',\n",
    "               'y' : 'ipsilon',\n",
    "               'z' : 'zeta',\n",
    "               'w' : 'vu'}\n",
    "\n",
    "IT_abbr = {'mr' : 'mister',\n",
    "            'tv' : 'tivu',\n",
    "           'dr' : 'dottor',\n",
    "           'ktml' : 'akkatiemmelle',\n",
    "           'tStSk': 'tSitSiakka',\n",
    "           'st' : 'seint',\n",
    "           'vs' : 'versus',\n",
    "           'mrs' : 'missis',\n",
    "           'jr' : 'dZunior',\n",
    "           'dj' : 'didZei',\n",
    "           'km' : 'kilometri',\n",
    "           'ms' : 'miss',\n",
    "           'dzld' : 'old'}\n",
    "\n",
    "           \n",
    "\n",
    "IT_others = {'skdZps' : 'skajp',\n",
    "             'diellecistoria' : 'dielletSistorja',\n",
    "             'btdZps' : 'bitajp',\n",
    "             'ktdZps' : 'tSitajp',\n",
    "             'sdZbersorElle' : 'sajbersorElle',\n",
    "             'sdZbErbullizmo': 'sajbErbullizmo',\n",
    "             'sdZkes' : 'sajkes',\n",
    "             'sdZrano' : 'sirano',\n",
    "             'sdZberterrorizmo' : 'sajberterrorizmo',\n",
    "             'sdZberpunk' : 'sajberpunk',\n",
    "             'sdZber' : 'sajber',\n",
    "             'sdZamalan' : 'Samalan',\n",
    "             'kk' : 'kejkej',\n",
    "             'kmr' : 'kmer',\n",
    "             'pk' : 'pikei',\n",
    "             'kz' : 'kozo',\n",
    "             'nsjk' : 'ansik',\n",
    "             'dzrt' : 'Sort',\n",
    "             'kl' : 'akkaelle',\n",
    "             'spdZksr' : 'spaiker',\n",
    "             'cfr' : 'confronta',\n",
    "             'mk' : 'emmekappa',\n",
    "             'gmbk' : 'dZiembieitS',\n",
    "             'nOn' : 'non',\n",
    "             'kmmevui' : 'kiwi',\n",
    "             'krtsisttof' : 'kristof',\n",
    "             'pdZrmont' : 'pirmont',\n",
    "             'wbleifozberi' : 'uebleifosberi',\n",
    "             'dbus' : 'dibus',\n",
    "             'dbre' : 'debre',\n",
    "             'dzrta' : 'orta',\n",
    "             'bdZpass' : 'bajpass',\n",
    "             'stSubert' : 'Subert',\n",
    "             'stSule': 'Sule',\n",
    "             'stSulttts' : 'Sults',\n",
    "             'stSulte' : 'Sulte',\n",
    "             'stSumaker' : 'Sumaker',\n",
    "             'stSults' : 'Sults',\n",
    "             'stSultse' : 'Sultse',\n",
    "             'stSuman' : 'Suman',\n",
    "             'krger' : 'kriger',\n",
    "             'tjvkezburi' : 'tiwkezberi',\n",
    "             'ksmen' : 'iksmen',\n",
    "             'mnkeis' : 'monkeis',\n",
    "             'mnkees' : 'monkis',\n",
    "             'mnkei' : 'monkei',\n",
    "             'sknidsr' : 'Snaider',\n",
    "             'sknabEl' : 'SnabEl',\n",
    "             'skneider' : 'Snaider',\n",
    "             'sknitsler' : 'Snitsler',\n",
    "             'skneeklOtks' : 'Sneeklots',\n",
    "             'mtSintdZrs' : 'mekintajrs',\n",
    "             'skmitts' : 'Smitts',\n",
    "             'skmitt' : 'Smitt',\n",
    "             'kkuaua' : 'tSiwawa',\n",
    "             'bmmevua' : 'biemmevu',\n",
    "             'mtSkkinlei' : 'mekkinlei',\n",
    "             'mtSkknigt' : 'mekknigt',\n",
    "             'mtSkke' : 'mekki',\n",
    "             'mtSkkenna' : 'mekkenna',\n",
    "             'mtSkkagan' : 'mekkagan',\n",
    "             'mtSkkentsje' : 'mekkentsje',\n",
    "             'mtSkkinnon' : 'mekkinnon',\n",
    "             'mtSilrOi' : 'mekilrOi',\n",
    "             'mtSintoS' : 'mekintoS',\n",
    "             'ksttsjbit' : 'eksibit',\n",
    "             'sttsabo' : 'dZabo',\n",
    "             'dlkstOrja' : 'diellecistoria',\n",
    "             'gstabilita' : 'astabilita',\n",
    "             'mskif' : 'miseSif',\n",
    "             'stdaddZer' : 'stritdager',\n",
    "             'dzbiJJiju' : 'dZibiJiev',\n",
    "             'vnka' : 'wonka',\n",
    "             'gdmi' : 'akkadiemmei',\n",
    "             'llswelin' : 'liwelin',\n",
    "             'dzdzEdd' : 'dZed',\n",
    "             'bdZrne' : 'bairne',\n",
    "             'dvjer' : 'dajer',\n",
    "             'dvOak' : 'dwoak',\n",
    "             'dvan' : 'dwan',\n",
    "             'dvigt' : 'dwaigt',\n",
    "             'dvains' : 'dwains',\n",
    "            #  'llobregat' : 'lobregat',\n",
    "            #  'llorEnte' : 'lorEnte',\n",
    "             'dllEden' : 'dellEden',\n",
    "            #  'llano' : 'lano',\n",
    "            #  'llOid' : 'lOid',\n",
    "            #  'lleiton' : 'leiton'\n",
    "             'tkenji' : 'tokendZi',\n",
    "             'mbour' : 'embour',\n",
    "             'mkbain' : 'mekbain',\n",
    "             'mkbEal' : 'mekbEal',\n",
    "             'gfunk' : 'dZifunk',\n",
    "             'mri' : 'emme erre i',\n",
    "             'mmevuenti' : 'emmeventi',\n",
    "             'dEllnksa ' : 'dEllenne tSi esse a',\n",
    "             'jvuvjoooo' : 'evovo',\n",
    "             'njvpOrt' : 'njuport',\n",
    "             'njvman' : 'njuman',\n",
    "             'njvgate' : 'njugeit',\n",
    "             'njvburg' : 'njuburg',\n",
    "             'gjpsi' : 'gipsi',\n",
    "             'lga' : 'ellegia',\n",
    "             'jjuels' : 'dZuels',\n",
    "             'jjuell' : 'dZuell', \n",
    "             'jjaOttsi' : 'jaOttsi',\n",
    "             'jjang' :'jang',\n",
    "             'njvmarket' : 'njumarket',\n",
    "             'dZjparana' : 'dZiparana',\n",
    "             'dZperjon' : 'ajperjon',\n",
    "             'pltsen' : 'psen',\n",
    "             'rvanda' : 'rwanda',\n",
    "             'prva' : 'prova',\n",
    "             'kdzrda' : 'korda',\n",
    "             'kdanSja' : 'kodanSa',\n",
    "             'mkrae' : 'mekrae',\n",
    "             'vbrake' : 'vibrejk',\n",
    "             'tdZpe' : 'tajpe',\n",
    "             'rdZerson' : 'rajerson',\n",
    "             'sjrus' : 'sirus',\n",
    "             'ndZberkssundtrizil' : 'najberkssundtrizil',\n",
    "             'mjron' : 'miron',\n",
    "             'bside' : 'abside',\n",
    "             'ksssoffsset' : 'iksoffset',\n",
    "             'mjju' : 'miu',\n",
    "             'mjjako' : 'miako',\n",
    "             'mjjadZi' : 'miadZi',\n",
    "             'sjprus' : 'siprus',\n",
    "             'ljra' : 'lira',\n",
    "             'fbi' : 'efbiai',\n",
    "             'mmmevuatalli' : 'muvatalli',\n",
    "             'srjuzburi' : 'Srjuzburi',\n",
    "             'dddZe' : 'dodZe',\n",
    "             'mkbain' : 'mekbain',\n",
    "             'kbab' : 'kebab',\n",
    "             'jjfua' : 'ipua',\n",
    "             'tminus' : 'timinus',\n",
    "             'gssu' : 'su',\n",
    "             'rjbakk' : 'ribakk',\n",
    "             'rmotorspOrt' : 'ErmotorspOrt',\n",
    "             'gwjbrus' : 'gajbruS',\n",
    "             'ldZons' : 'lajons',\n",
    "             'jjujitsu' : 'dZudZitsu',\n",
    "             'dkelp' : 'Elp',\n",
    "             'ksbOks' : 'iksbOks',\n",
    "             'kjjoSSi' : 'kjoSSi',\n",
    "             'fjrstenbErg' : 'firstenbErg',\n",
    "             'gveda' : 'rigveda',\n",
    "             'tjrrEll' : 'tirrEll',\n",
    "             'tjra' : 'tjra',\n",
    "             'skglesindZer' : 'SlesiNger',\n",
    "             'kssenuja' : 'ksenudZa',\n",
    "             'skvabintvest' : 'Svabingvest',\n",
    "             'skvartseneddZer' : 'SvartseneddZer',\n",
    "             'skvartsenbErg' : 'SvartsenbErg',\n",
    "             'skvarman' : 'Svarman',\n",
    "             'skverin' : 'Sverin',\n",
    "             'skvab' : 'Svab',\n",
    "             'bjron' : 'bajron',\n",
    "             'ndimensjonali' : 'enne dimensjonali',\n",
    "             'ndimensjonale' : 'enne dimensjonale',\n",
    "             'lnezimo' : 'lennezimo',\n",
    "             'dleagwe' : 'dilig',\n",
    "             'stpanek' : 'stepanek',\n",
    "             'dZkjad' : 'dZiad',\n",
    "             'kttserolungo' : 'kappa dzero breve',\n",
    "             'kttserobreve' : 'kappa dzero luNgo',\n",
    "             'ktan' : 'katan',\n",
    "             'mkaulei' : 'mekaulei',\n",
    "             'mkfErson' : 'mekfErson',\n",
    "             'trble' : 'trebol',\n",
    "             'spdZem' : 'spajem',\n",
    "             'ksfiles' : 'iksfiles',\n",
    "             'ksfaktor' : 'iksfaktor',\n",
    "             'gdinja' : 'gidinja',\n",
    "             'tsssOlt' : 'tsOlt',\n",
    "             'tjbreak' : 'tibrejk',\n",
    "             'mstizlav' : 'mstizlav',\n",
    "             'dzli' : 'oli',\n",
    "             'ktsestokowa' : 'tSestokova',\n",
    "             'tnkeani' : 'tonkeani',\n",
    "             'tnkean' : 'tonkean',\n",
    "             'bkar' : 'biar',\n",
    "             'mliss' : 'emliss',\n",
    "             'tjrrEll' : 'tirrEll',\n",
    "             'mdZers' : 'majers',\n",
    "             'flnom' : 'pnom',\n",
    "             'kfukuji' : 'kofukuji',\n",
    "             'dzrinski' : 'dZarinski',\n",
    "             'tdZke' : 'tajk',\n",
    "             'ppErelakaise' : 'pErelakaise',\n",
    "             'ppEre' : 'pEre',\n",
    "             'lleiton': 'leiton',\n",
    "              'lluis': 'luis',\n",
    "              'llOid': 'lOid',\n",
    "              'llorEnte': 'lorEnte',\n",
    "              'llanOs': 'lanOs',\n",
    "              'llobregat': 'lobregat',\n",
    "              'llano': 'lano',\n",
    "              'ssukamoto': 'sukamoto',\n",
    "              'ssujoSSi': 'sujoSSi',\n",
    "              'sserdZo': 'serdZo',\n",
    "              'ssebastjen': 'sebastjen',\n",
    "              'ssukaza': 'sukaza',\n",
    "              'ssjEnko': 'sjEnko',\n",
    "              'ssunami': 'sunami',\n",
    "              'sserje': 'serje',\n",
    "              'mmevuain': 'mevuain',\n",
    "              'mmevuitking': 'mevuitking',\n",
    "              'mmevuaddZer': 'mevuaddZer',\n",
    "              'mmevuinkle': 'mevuinkle',\n",
    "              'mmevuins': 'mevuins',\n",
    "              'mmevuerks': 'mevuerks',\n",
    "              'mmevuanson': 'mevuanson',\n",
    "              'mmevuimming': 'mevuimming',\n",
    "              'mmevuin': 'mevuin',\n",
    "              'mmevueet': 'mevueet',\n",
    "              'mmevuente': 'mevuente',\n",
    "              'mmevangkol': 'mevangkol',\n",
    "              'mmevuelve': 'mevuelve',\n",
    "              'mmevue': 'mevue',\n",
    "              'mmevuaimir': 'mevuaimir',\n",
    "              'mmevuitter': 'mevuitter',\n",
    "              'mmevuedenborg': 'mevuedenborg',\n",
    "              'mmevuartts': 'mevuartts',\n",
    "              'mmevuista': 'mevuista',\n",
    "              'mmevuitts': 'mevuitts',\n",
    "              'mmevuisted': 'mevuisted',\n",
    "              'mmevuim': 'mevuim',\n",
    "              'mmevang': 'mevang',\n",
    "              'mmevuing': 'mevuing',\n",
    "              'mmevuarr': 'mevuarr',\n",
    "              'mmevuattsiland': 'mevuattsiland',\n",
    "              'mmevuan': 'mevuan',\n",
    "              'mmevuiss': 'mevuiss',\n",
    "              'mmevuate': 'mevuate',\n",
    "              'mmevuifttOjOta': 'mevuifttOjOta',\n",
    "              'mmevinedd': 'mevinedd',\n",
    "              'mmevuentjEt': 'mevuentjEt',\n",
    "              'mmevuifmmeven': 'mevuifmmeven',\n",
    "              'mmevuo': 'mevuo',\n",
    "              'mmevuentjone': 'mevuentjone',\n",
    "              'mmeva': 'meva',\n",
    "              'mmevuajili': 'mevuajili',\n",
    "              'mmevuings': 'mevuings',\n",
    "              'mmevuitk': 'mevuitk',\n",
    "              'mmevuein': 'mevuein',\n",
    "              'mmevai': 'mevai',\n",
    "              'mmeveetveetvee': 'meveetveetvee',\n",
    "              'mmevuittano': 'mevuittano',\n",
    "              'mmevingli': 'mevingli',\n",
    "              'mmevuOrd': 'mevuOrd',\n",
    "              'mmevueep': 'mevueep',\n",
    "              'SSiimittsu': 'Siimittsu',\n",
    "              'SSiirlei': 'Siirlei',\n",
    "              'SSiinano': 'Siinano',\n",
    "              'SSiiddzuka': 'Siiddzuka',\n",
    "              'SSiield': 'Siield',\n",
    "              'SSiiniki': 'Siiniki',\n",
    "              'SSiittswoka': 'Siittswoka',\n",
    "              'SSiippingkompani': 'Siippingkompani',\n",
    "              'SSiigure': 'Siigure',\n",
    "              'SSiia': 'Siia',\n",
    "              'SSiitwok': 'Siitwok',\n",
    "              'mis@Sif' : 'misESif',\n",
    "              'SSiin': 'Siin',\n",
    "              'SSiields': 'Siields',\n",
    "              'SSiirai': 'Siirai',\n",
    "              'SSiipton': 'Siipton',\n",
    "              'SSiip': 'Siip',\n",
    "              'SSiifu': 'Siifu',\n",
    "              'SSiira': 'Siira',\n",
    "              'SSiinkansen': 'Siinkansen',\n",
    "              'SSiining': 'Siining',\n",
    "              'SSiikari': 'Siikari',\n",
    "              'SSiiki': 'Siiki',\n",
    "              'SSiimattsu': 'Siimattsu',\n",
    "              'SSiiva': 'Siiva',\n",
    "              'SSiinee': 'Siinee',\n",
    "              'SSiindzon': 'Siindzon',\n",
    "              'SSiiina': 'Siiina',\n",
    "              'SSiibuja': 'Siibuja',\n",
    "              'SSiimsaiki': 'Siimsaiki',\n",
    "              'SSiire': 'Siire',\n",
    "              'SSiipit': 'Siipit',\n",
    "              'SSiiro': 'Siiro',\n",
    "              'SSiinagava': 'Siinagava',\n",
    "              'SSiir': 'Siir',\n",
    "              'SSiips': 'Siips',\n",
    "              'SSiif': 'Siif',\n",
    "              'SSiinbun': 'Siinbun',\n",
    "              'SSiindZen': 'SiindZen',\n",
    "              'SSiirakavaiki': 'Siirakavaiki',\n",
    "              'SSiirats': 'Siirats',\n",
    "              'SSiiujuku': 'Siiujuku'\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function\n",
    "\n",
    "IT_train_df['phonl_tr'] = IT_train_df['phonl_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])\n",
    "IT_train_df['phont_tr'] = IT_train_df['phont_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])\n",
    "IT_test_df['phonl_tr'] = IT_test_df['phonl_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])\n",
    "IT_test_df['phont_tr'] = IT_test_df['phont_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])\n",
    "IT_val_df['phonl_tr'] = IT_val_df['phonl_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])\n",
    "IT_val_df['phont_tr'] = IT_val_df['phont_tr'].apply(fix_transcriptions, args=[IT_spelling, IT_abbr, IT_others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITdataset = pd.concat([IT_train_df, IT_test_df, IT_val_df],  ignore_index=True)\n",
    "\n",
    "ITdataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving fixed transcriptions\n",
    "%cd /mydir\n",
    "\n",
    "pkl_name = f\"fix_itCV_df.pkl\"\n",
    "with open(pkl_name, 'wb') as file:\n",
    "  # A new file will be created\n",
    "  pkl.dump([IT_train_df, IT_test_df, IT_val_df], file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train\n",
    "ES_train_df['phonl_tr'] = ES_train_df['phonl_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "ES_train_df['phont_tr'] = ES_train_df['phont_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "\n",
    "#test\n",
    "ES_test_df['phonl_tr'] = ES_test_df['phonl_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "ES_test_df['phont_tr'] = ES_test_df['phont_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "\n",
    "\n",
    "# #val\n",
    "ES_val_df['phonl_tr'] = ES_val_df['phonl_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "ES_val_df['phont_tr'] = ES_val_df['phont_tr'].apply(transcrCleanerMltLng, args=['es'])\n",
    "\n",
    "\n",
    "ESacr_train = find_acronyms(ES_train_df) #195\n",
    "ESacr_test = find_acronyms(ES_test_df) #55\n",
    "ESacr_val = find_acronyms(ES_val_df) #37\n",
    "\n",
    "\n",
    "# list of ES problematic transcriptions\n",
    "\n",
    "ESall_probs = [el[1] for el in ESacr_test + ESacr_train + ESacr_val]\n",
    "ESall_probs_flat = [w for el in ESall_probs for w in el]\n",
    "\n",
    "\n",
    "# set of ES problematic transcriptions\n",
    "\n",
    "ESall_probs_s = list(set(ESall_probs_flat))\n",
    "ESall_probs_s # 63 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language-dependent vocabularies to fix the issues\n",
    "\n",
    "ES_spelling = {'b' : 'be', \n",
    "               'B' : 'Be',\n",
    "               'tS' : 'tSe',\n",
    "               'ts' : 'tse', \n",
    "               'ttS' : 'tSe',\n",
    "               'd' : 'de',\n",
    "               'D' : 'de',\n",
    "               'f' : 'efe',\n",
    "               'dZ' : 'dZe',\n",
    "               'dz' : 'dze',\n",
    "               'g' : 'xe', \n",
    "               'G' : 'Ge',\n",
    "               'h' : 'atSe', \n",
    "               'j' : 'xota',\n",
    "               'k' : 'ka',\n",
    "               'l' : 'ele',\n",
    "               'm' : 'eme',\n",
    "               'n' : 'ene',\n",
    "               'p' : 'pe',\n",
    "               'q' : 'ku',\n",
    "               'r' : 'ere',\n",
    "               's' : 'ese',\n",
    "               't' : 'te',\n",
    "               'T' : 'Te',\n",
    "               'v' : 'uve',\n",
    "               'x' : 'ekis',\n",
    "               'z' : 'seta',\n",
    "               'w' : 'udoBle'\n",
    "               }\n",
    "\n",
    "ES_abbr = {'mr' : 'mister',\n",
    "           'tv' : 'teve',\n",
    "           'dr' : 'doktor',\n",
    "           'bs' : 'boliBares',\n",
    "           'st' : 'seint',\n",
    "           'vs' : 'versus',\n",
    "           'mrs' : 'missis',\n",
    "           'xr' : 'dZunior', ### VARIATION!\n",
    "           'dj' : 'didZei',\n",
    "           'ms' : 'miss'}\n",
    "\n",
    "           \n",
    "\n",
    "ES_others = {'ls' : 'las',\n",
    "             'mkxee ' : 'makii',\n",
    "             'fljjn' : 'flin',\n",
    "             'prt' : 'part',\n",
    "             'ljjn' : 'lin',\n",
    "             'ljjNk' : 'link',\n",
    "             'mtn' : 'mawne',\n",
    "             'iron' : 'airon',\n",
    "             'bjjrDs' : 'birds',\n",
    "             'tx' : 'tojo',\n",
    "             'kf' : 'seefe',\n",
    "             'kn' : 'kaene',\n",
    "             'bxrk' : 'bSork',\n",
    "             'mn' : 'm',\n",
    "             'rrjj' : 'rii',\n",
    "             'rrjjs' : 'riis',\n",
    "             'rrjjtn' : 'ritn',\n",
    "             'rrjjkjj' : 'riki',\n",
    "             'gf' : 'gief',\n",
    "             'mjjst' : 'miist',\n",
    "             'mGs' : 'emdZies',\n",
    "             'ft' : 'fit',\n",
    "             'pjjn' : 'pin',\n",
    "             'bljjt' : 'blit',\n",
    "             'kT' : 'seDe',\n",
    "             'gjjpzjj' : 'dZispi',\n",
    "             'xjjBskjjl' : 'xiBaskila',\n",
    "             'kl' : 'sei',\n",
    "             'jjs' : 'is',\n",
    "             'lk' : 'ele iGual a ka' ,\n",
    "             'kDx' : 'seDexe',\n",
    "             'tjjk' : 'tik',\n",
    "             'gjjr' : 'gior',\n",
    "             'ms' : 'mas',\n",
    "             'bjjrT' : 'birT',\n",
    "             'jjldT' : 'dZildZis',\n",
    "             'bksx' : 'vaxo',\n",
    "             'tkl' : 'te se ele',\n",
    "             'jjt' : 'ite',\n",
    "             'tG' : 'toGo',\n",
    "             'gwjjnet' : 'gwinet',\n",
    "             'ntra' : 'nwestra',\n",
    "             'pjjLis' : 'fillis',\n",
    "             'tSteaw' : 'tSatow',\n",
    "             'tSteawDks' : 'tSatow deks',\n",
    "             'gnostiTizmo' : 'agnostiTizmo',\n",
    "             'gnekko' : 'nekko',\n",
    "             'xnjo' : 'dZanjo',\n",
    "             'TwaTwa' : 'suasua',\n",
    "             'mkpee' : 'mekpi',\n",
    "             'wjjman' : 'wiman',\n",
    "             'ljjl' : 'lajl',\n",
    "             'stSsistSe' : 'SaSeSwetS',\n",
    "             'mburukujja' : 'burukujja',\n",
    "             'mGo' : 'maGo',\n",
    "             'mNgk' : 'moNke',\n",
    "             'rrtjjpe' : 'erretaip',\n",
    "             'tSrjjzler' : 'krizler',\n",
    "             'bstra' : 'bastra',\n",
    "             'gtaland' : 'gotaland',\n",
    "             'kfir' : 'kafir',\n",
    "             'djjnasti' : 'dinasti',\n",
    "             'djjson' : 'dajson',\n",
    "             'djjnamo' : 'dinamo',\n",
    "             'djjke' : 'dik',\n",
    "             'djjer' : 'dajer',\n",
    "             'djjlan' : 'dilan',\n",
    "             'kljjDe' : 'klajDe',\n",
    "             'fljjiNg' : 'flaiNg',\n",
    "             'bjjtekoDe' : 'baitekoDe',\n",
    "             'bjjtes' : 'baites',\n",
    "             'bjjnun' : 'binun',\n",
    "             'bjje' : 'bai',\n",
    "             'bjjrne' : 'birne',\n",
    "             'bjjpas' : 'bajpas',\n",
    "             'bjjron' : 'bajron',\n",
    "             'bjjuNgun' : 'biuNgun',\n",
    "             'ljjman' : 'liman',\n",
    "             'ljjme' : 'lime',\n",
    "             'kBote' : 'kote',\n",
    "             'kBe' : 'koBe',\n",
    "             'mkBriDe' : 'makBriDe',\n",
    "             'kBa' : 'kuBa',\n",
    "             'mkkinon' : 'makkinon', \n",
    "             'mkkinsej': 'makkinsei',\n",
    "             'mkkorT' : 'makkorT',\n",
    "             'mkkeLar' :  'makkeLar', \n",
    "             'mkkaGan': 'mekkaGan',\n",
    "             'mkklajn' : 'makklajn',\n",
    "             'mkkuneorejstSawer' : 'makkuneorejSawer',\n",
    "             'mkkartnej' : 'mekkartnej',\n",
    "             'mkkan' : 'makkan',\n",
    "             'mkkaj': 'makkaj',\n",
    "             'mkkoj' : 'makkoj',\n",
    "             'mkkuLotS' : 'makkuLotS',\n",
    "             'mkkonawGej' : 'makkonawGej', \n",
    "             'mkkuLowG' : 'makkuLowG', \n",
    "             'mkkormikk' : 'makkormikk', \n",
    "             'mkkandles' : 'makkandles',\n",
    "             'mkkarti' : 'makkarti',\n",
    "             'rrjjan' : 'rrajan', \n",
    "             'rrjju' : 'rriu', \n",
    "             'rrjjo' : 'rrio', \n",
    "             'rrjjoko' : 'rrooko', \n",
    "             'rrjjwitSi' : 'rriuitSi', \n",
    "             'rrjjDBerx': 'rruiDBerx',\n",
    "             'rrjjle' : 'rrile' ,\n",
    "              'rrjjoej' : 'rrioxej',\n",
    "              'rrjjukjju' : 'rriukju',\n",
    "              'rrjjoNgae' : 'rrioNgae', \n",
    "             'rrjjota' : 'rriota', \n",
    "             'rrjjuxi' : 'rriuxi',\n",
    "             'ljjle' : 'lile', \n",
    "             'jjlaNgjjlaNg' : 'ilaNgilaNg', \n",
    "             'kjjla' : 'kila', \n",
    "             'tjjler' : 'tajler', \n",
    "             'pjjle' : 'pile',\n",
    "             'kjjlje' : 'kailje',\n",
    "             'kjjle' : 'kaile', \n",
    "             'mjjles' : 'miles',\n",
    "             'krjjloB' : 'kriloB',\n",
    "             'mjjkoplazma': 'mikroplazma',\n",
    "             'mjjka' : 'mika',\n",
    "             'dformes' : 'deformes',\n",
    "             'pjjro' : 'piro',\n",
    "             'pjjsiks' : 'fisiks',\n",
    "             'pjjoNgjjaNg' : 'pioNgiaNg',\n",
    "             'pjjton' : 'piton',\n",
    "             'pjjsikal' : 'fisikal',\n",
    "             'JjjkpiNg' : 'JikopiNg',\n",
    "             'mkaDoo' : 'makaDoo',\n",
    "             'mklaren' : 'maklaren',\n",
    "             'mkDonalds' : 'makDonalds',\n",
    "             'mknamara' : 'maknamara',\n",
    "             'mkBeal' : 'makBeal',\n",
    "             'mkmurDo' : 'makmurDo',\n",
    "             'mkGoBern' : 'makGoBern',\n",
    "             'mkmaon' : 'makmaon',\n",
    "             'mkDoneL' : 'makDoneL',\n",
    "             'mkGee' : 'makGee',\n",
    "             'mklowGlin' : 'maklowGlin',\n",
    "             'mkBriDe' : 'makBriDe',\n",
    "             'mkDermot' : 'makDermot',\n",
    "             'mkGreGor' : 'makGreGor',\n",
    "             'mkrae'  : 'makrae',\n",
    "             'mkmanus' : 'makmanus',\n",
    "             'mkDowGaL' : 'makDowGaL',\n",
    "             'mklean' : 'maklean',\n",
    "             'mkDonald' : 'makDonald',\n",
    "             'mkpee' : 'makpi',\n",
    "             'mkxee' : 'magii',\n",
    "             'mkaLen' : 'makaLen',\n",
    "             'djjke' : 'dike',\n",
    "             'kjjril' : 'siril',\n",
    "             'tSwa' : 'tSua',\n",
    "             'tSwan' : 'tSuan',\n",
    "             'kjjprus' : 'siprus',\n",
    "             'kjjBorx' : 'sajBor',\n",
    "             'kjjkliNg' : 'sikliNg',\n",
    "             'kjjmru' : 'simru',\n",
    "             'kjjklokros' : 'siklokros',\n",
    "             'kjjklone' : 'siklone',\n",
    "             'kjjuSu' : 'kjuSu',\n",
    "             'kjjoko' : 'kioko',\n",
    "             'kjjBertron' : 'siBertron',\n",
    "             'kjjGnus' : 'siGnus',\n",
    "             'dpto' : 'departamento',\n",
    "             'brjjTe' : 'briTe',\n",
    "             'fxorDane' : 'fjorDein',\n",
    "             'ktSi' : 'kotSi',\n",
    "             'Jjjmburk' : 'Jimburk',\n",
    "             'jjriGojjen' : 'iriGojen',\n",
    "             'mjjrtle' : 'mirtle',\n",
    "             'gjjrGi' : 'gjorgi',\n",
    "             'ljjra' : 'lira',\n",
    "             'tjjrjon' : 'tirjon',\n",
    "             'kjjril' : 'kiril',\n",
    "             'mjjrmeTiTa' : 'mirmeTiTa',\n",
    "             'nrro' : 'numero',\n",
    "             'mNkeNglaDBatS' : 'moNxeNglaBax',\n",
    "             'jjNgxik' : 'joNgSi',\n",
    "             'jjpres' : 'ipres',\n",
    "             'jjko' : 'iko',\n",
    "             'tjjkoon' : 'tikun',\n",
    "             'mjjka' : 'mika',\n",
    "             'rtel' : 'xertel',\n",
    "             'gDriBe' : 'dZiDraiBe',\n",
    "             'bSork' : 'bSork',\n",
    "             'pTja' : 'provinsia',\n",
    "             'kTerni' : 'serni',\n",
    "             'krjjTek' : 'krisek',\n",
    "             'mnet' : 'emnet',\n",
    "             'brjjant' : 'brajant',\n",
    "             'brjjTe' : 'briTe',\n",
    "             'rrjjle' : 'rrile',\n",
    "             'bxrkman' : 'borkman',\n",
    "             'lselektina' : 'elselektina',\n",
    "             'wwwsfGatekon' : 'dobleve dobleve dobleve punto ese efe geit punto kom',\n",
    "             'krjjstal' : 'kristal',\n",
    "             'tBjjnoBelas' : 'teve i noBelas',\n",
    "             'mxolnir' : 'mjolnir',\n",
    "             'krjjtek' : 'kritek',\n",
    "             'dnis' : 'denis',\n",
    "             'ljjDja' : 'liDja',\n",
    "             'tkon' : 'tekom',\n",
    "             'lnder' : 'linder',\n",
    "             'jjutaka' : 'iutaka',\n",
    "              'jjojjo' : 'iojo',\n",
    "              'jjuNg' : 'iuNg',\n",
    "              'jji' : 'ji',\n",
    "              'jjat' : 'iat', \n",
    "             'jjiNg' : 'jiNg',\n",
    "             'jjaropolk' : 'iaropolk', \n",
    "             'jjunta' : 'iunta',\n",
    "             'jjukari' : 'iukari', \n",
    "             'jjari' : 'iari',\n",
    "             'jjoSiiro' : 'ioSiiro', \n",
    "             'jjarakuj' : 'iarakuj', \n",
    "             'jjaGwe' : 'iaGwe', \n",
    "             'jjeLowxakket' : 'jeLowxakket', \n",
    "             'jjon' : 'ion', \n",
    "             'jjamaSita' : 'jamaSita', \n",
    "             'jjelos' : 'ielos', \n",
    "             'jjwiNgs' : 'dZiwiNgs',\n",
    "             'jjamawtSi' : 'iamawtSi',\n",
    "             'jjerma' : 'ierma', \n",
    "             'jjuka' : 'iuka',\n",
    "             'jjarozlaB' : 'iarozlaB',\n",
    "             'jjankee' : 'janki', \n",
    "             'jjel' : 'iel', \n",
    "             'jjaTen' : 'jaTen', \n",
    "             'jjates' : 'jates',\n",
    "             'jjuko' : 'juko', \n",
    "             'jjaGo' : 'jaGo', \n",
    "             'jjap' : 'jap', \n",
    "             'jjowtuBe' : 'jowtuBe',\n",
    "             'jjamuna' : 'jamuna', \n",
    "             'jjoSijjuki' : 'joSijuki', \n",
    "             'jjupaNki' : 'jupaNki', \n",
    "             'jjonkers' : 'jonkers', \n",
    "             'jjori' : 'jori',\n",
    "             'jjerro' : 'jerro', \n",
    "             'jjamaa' : 'jamaa', \n",
    "             'jjusej' : 'jusei', \n",
    "             'jjaTimjento' : 'jaTimjento', \n",
    "             'jjaTe' : 'jaTe',\n",
    "             'jjman' : 'iman',\n",
    "             'jjaTiDies' : 'jaTiDies', \n",
    "             'jjandel' : 'jandel', \n",
    "             'jjuDiStira' : 'juDiStira',\n",
    "             'jjokasta' : 'jokasta',\n",
    "             'jjunsoo' : 'junsoo', \n",
    "             'jjoo' : 'joo',\n",
    "             'jjamane' : 'jamane',\n",
    "             'jjukje' : 'jukje', \n",
    "             'jjukje' : 'jukje', \n",
    "             'jjeBxeni' : 'jeBxeni', \n",
    "             'jjuGozlaBo' : 'juGozlaBo',\n",
    "             'jjasuSi' : 'jasuSi',\n",
    "             'jjear' : 'iar', \n",
    "             'jje' : 'je', \n",
    "             'jjera' : 'jera', \n",
    "             'jjerra' : 'jerra', \n",
    "             'jjarozlaBl' : 'jarozlaBl',\n",
    "             'jjorkk' : 'jorkk',\n",
    "             'jjaBe' : 'jaBe',\n",
    "             'jjale' : 'jale',\n",
    "             'jjuGozlaBa' : 'juGozlaBa', \n",
    "             'jjes' : 'jes', \n",
    "             'jjeso' : 'jeso', \n",
    "             'jjo' : 'jo', \n",
    "             'jja' : 'ja',\n",
    "             'jjowrself' : 'iowrself', \n",
    "             'jjazmine' : 'jazmine',\n",
    "             'jjamatanoorotSi' : 'jamatanorotSi',\n",
    "             'jjBete' : 'jBete', \n",
    "             'jjeLow' : 'jeLow', \n",
    "             'jjBriT' : 'jBriT', \n",
    "             'jjolanda' : 'jolanda', \n",
    "             'jjakoBleB' : 'jakoBleB', \n",
    "             'jjoSimitsu' : 'joSimitsu', \n",
    "             'jjajja' : 'jajja', \n",
    "             'jjuDo' : 'juDo',\n",
    "             'jjenes' : 'jenes', \n",
    "             'jjani' : 'jani',\n",
    "             'jjaNki' : 'jaNki', \n",
    "             'jjakarta' : 'dZakarta', \n",
    "             'jjenas' : 'jenas',\n",
    "             'jjandeks' : 'jandeks', \n",
    "             'jjwan' : 'iwan', \n",
    "             'jjena' : 'jena', \n",
    "             'jjamaTaki' : 'jamaTaki', \n",
    "             'jjekaterina' : 'jekaterina',\n",
    "             'jjiaT' : 'jiaT', \n",
    "             'jjoGi' : 'joGi', \n",
    "             'jjelo' : 'ielo',\n",
    "             'jju' : 'iu', \n",
    "             'jjasukuni' : 'jasukuni',\n",
    "             'jjukatan' : 'jukatan',\n",
    "             'jjonrrog' : 'jonrrog', \n",
    "             'jjanes': 'janes', \n",
    "             'jjemeni' : 'jemeni',\n",
    "             'jjkosan': 'ikosan',\n",
    "             'jjeoNgu' : 'jeoNgu',\n",
    "             'jjeats' : 'jeats',\n",
    "             'jjuNxin': 'juNxin', \n",
    "             'jjajTa' : 'jajTa', \n",
    "             'jjuki' : 'juki',\n",
    "             'jjelmo' : 'jelmo',\n",
    "             'jjeBra': 'jeBra', \n",
    "             'jjoruBa' : 'joruBa',\n",
    "             'jjuGozlaBja' : 'juGozlaBja', \n",
    "             'jjarTa' : 'jarTa', \n",
    "             'jjow' : 'jow', \n",
    "             'jjorkSire' : 'jorkSire', \n",
    "             'jjerBas' : 'jerBas',\n",
    "             'jjowtuBers' : 'jowtuBers',\n",
    "             'jjone' : 'jone', \n",
    "             'jjurijjeBpolski' : 'jurijjeBpolski', \n",
    "             'jjaGisijjan' : 'jaGisijjan', \n",
    "             'jjpe' : 'ipe', \n",
    "             'jjuTni' : 'juTni', \n",
    "             'jjuste' : 'juste',\n",
    "             'jjamato' : 'jamato', \n",
    "             'jjoSiki' : 'joSiki', \n",
    "             'jjawri' : 'jawri',\n",
    "             'jjeti' : 'jeti',\n",
    "             'jjuGozlaBos' : 'juGozlaBos', \n",
    "             'jjaGami' : 'jaGami', \n",
    "             'jjerGe' : 'jerGe', \n",
    "             'jjaTimjentos' : 'dZaTimjentos', \n",
    "             'jjunan' : 'junan',\n",
    "             'jjankees' : 'jankees',\n",
    "             'jjork':'jork', \n",
    "             'jjak': 'jak',\n",
    "             'jjamila' : 'jamila', \n",
    "             'jjet' : 'jet',\n",
    "             'jjeserias' : 'jeserias',\n",
    "             'jjenin' : 'jenin',\n",
    "             'jjorker' : 'jorker', \n",
    "             'jjunus' : 'junus',\n",
    "             'jjajjo' : 'jajjo', \n",
    "             'jjere' : 'jjre', \n",
    "             'jjowNger':'jowNger',\n",
    "             'jjoSiDa' : 'joSiDa',\n",
    "             'jjaser' : 'jaser', \n",
    "             'jjaoo' : 'jau', \n",
    "             'jjeas' : 'jeas', \n",
    "             'jjusuke' : 'jusuke',\n",
    "             'jjowNg' : 'jowNg', \n",
    "             'jjuGo' : 'juGo', \n",
    "             'jjamatoe' : 'jamatoe',\n",
    "             'jjema' : 'jema', \n",
    "             'jjuw' : 'juw', \n",
    "             'jjeGwas' : 'jeGwas', \n",
    "             'jjea' : 'jea', \n",
    "             'jjoGur' : 'joGur',\n",
    "             'jjermos': 'jermos',\n",
    "             'jjakuTa' : 'jakuTa',\n",
    "             'jjamaDa' : 'jamaDa',\n",
    "             'jjBone' : 'iBone', \n",
    "             'jjao' : 'jao',\n",
    "             'jjeroJjjmus' : 'jeroJjjmus',\n",
    "             'jjamal' : 'jamal', \n",
    "             'jjee' : 'jee', \n",
    "             'jjstejn' : 'istejn',\n",
    "             'jjowt' : 'jowt', \n",
    "             'jjpres' : 'ipres', \n",
    "             'jjerBen' : 'jerBen', \n",
    "             'jjuNgaj' : 'juNgaj', \n",
    "             'jjerBe' : 'jerBe',\n",
    "             'jjwi' : 'iwi',\n",
    "             'jjukoaTteka' :'jukoaTteka',\n",
    "             'jjoGa' : 'joGa', \n",
    "             'jjriGojjen' : 'iriGojjen', \n",
    "             'jjukimura' :'jukimura', \n",
    "             'jjuna' : 'juna',\n",
    "             'jjakis' : 'jakis', \n",
    "             'jjuNxoNg' : 'juNxoNg',\n",
    "             'jjoDo' : 'joDo',\n",
    "             'jjamaGutSi' : 'jamaGutSi', \n",
    "             'jjuxjo' : 'juxjo', \n",
    "             'jjuma' : 'juma', \n",
    "             'jjorDan' : 'jorDan', \n",
    "             'jjamena' : 'jamena', \n",
    "             'jjan' : 'jan',\n",
    "             'jjDro': 'iDro', \n",
    "             'jjepes' : 'jepes', \n",
    "             'jjazmin' : 'dZazmin', \n",
    "             'jjuri' : 'juri', \n",
    "             'jjDe': 'iDe', \n",
    "             'jjoko' : 'joko',\n",
    "             'jjamaSiro': 'jamaSiro', \n",
    "             'jjoma' : 'joma', \n",
    "             'jjNgxik' : 'iNgxik', \n",
    "             'jjaNgtSe' : 'jaNgtSe', \n",
    "             'jjarT' : 'jarT', \n",
    "             'jjeoBil' : 'jeoBil', \n",
    "             'jjemas': 'jemas', \n",
    "             'jjiSuB' : 'iSuB', \n",
    "             'jjomo' : 'jomo', \n",
    "             'jjanan' : 'janan', \n",
    "             'jjule' : 'jule', \n",
    "             'jjuriDja' : 'juriDja',\n",
    "             'jjonson' : 'dZonson',\n",
    "             'jjokoama' : 'jokoama',\n",
    "             'jjukatekas' : 'jukatekas',\n",
    "             'jjiButi': 'iButi', \n",
    "             'jjeoNxeGu' : 'jeoNxeGu',\n",
    "             'jjendo' : 'jendo',\n",
    "             'jjerBa' : 'jerBa', \n",
    "             'jjoNgle' : 'joNgle', \n",
    "             'jjokoTuna' : 'jokoTuna', \n",
    "             'jjate' : 'jate', \n",
    "             'jjoDuro':'joDuro', \n",
    "             'jjowr' : 'jowr',\n",
    "             'ltDa' : 'eletedea',\n",
    "             'dBarBara': 'BarBara',\n",
    "             'dBin' : 'deBin',\n",
    "             'dBina' : 'deBina',\n",
    "             'dBoak' : 'deBoak',\n",
    "             'Llamaron' : 'Lamaron',\n",
    "             'mkGoBern' : 'mekGoBern',\n",
    "             'krTjjsTtof' : 'krisTtof',\n",
    "             'gsta' : 'gosta',\n",
    "             'gstaaT' : 'gostaaT',\n",
    "             'lkDo' : 'elkaDo',\n",
    "             'TBiGnjew' : 'zpikNw',\n",
    "             'mjjsterjon' : 'misterjon',\n",
    "              'mjjanmar' : 'mianmar', \n",
    "             'mjjsterjous' : 'misterjous',\n",
    "             'mjjsterjo' : 'misterjo', \n",
    "             'mjjuNgwun' : 'mjuNgwun', \n",
    "             'mjjers' : 'miers', \n",
    "             'mjjseartSkomaw' : 'miseartSkomaw', \n",
    "             'mjjsteri' : 'misteri',\n",
    "             'mjjstik' : 'mistik', \n",
    "             'mjjstike' : 'mistike',\n",
    "             'wjjne' : 'wine',\n",
    "             'rrjjDBerx' : 'rriDBerx',\n",
    "             'rTte' : 'artSe',\n",
    "             'mlnik' : 'melnik',\n",
    "             'dto' : 'deteo',\n",
    "             'pBro' : 'prezbitero',\n",
    "             'brtjanu' : 'bratjanu',\n",
    "             'Ngema' :'Nengema',\n",
    "             'ljjndon' : 'lindon',\n",
    "             'JjjBorx' : 'sajBorx',\n",
    "             'xmin' : 'eksmin',\n",
    "             'kjjGnus' : 'signus',\n",
    "             'wjjomiNg' : 'wjomiNg',\n",
    "             'xrok' : 'xotarok',\n",
    "             'xruStSoB' : 'kruStSoB',\n",
    "             'mtlej' : 'motlej',\n",
    "             'wrokaw' : 'Brokav',\n",
    "             'wriGt' : 'rait',\n",
    "             'wrajt' : 'rajt',\n",
    "             'wren' : 'ren',\n",
    "             'wwweleNgaNkees' : 'doble Be doble Be doble Be punto eleNgaNtSe punto es',\n",
    "             'wwwfreesoftorx' : 'udoble udoble udoble frisoftorks',\n",
    "             'wwwTurGajkon' : 'doble Be doble Be doble Be punto surGaj punto kom',\n",
    "             'tjjsen' : 'tisen',\n",
    "             'tjjsembornemisTa' : 'tisembornemisTa',\n",
    "             'mknamara' : 'maknamara',\n",
    "             'Sxi' : 'Si',\n",
    "             'Sxo' : 'So',\n",
    "             'rrmoT' : 'erremoD',\n",
    "             'rrtrut' : 'ertrut',\n",
    "             'kta' : 'kota',\n",
    "             'brseNkowrjer' : 'borseNkowrjer',\n",
    "             'xnkp' : 'dZonkopinG',\n",
    "             'Nxal' : 'Nial',\n",
    "             'xxer' : 'xer',\n",
    "             'rrTa' : 'erreTea',\n",
    "             'jjDro' : 'iDro',\n",
    "             'ljjon' : 'ljon',\n",
    "             'ljjonsajntpawl' : 'ljonsajntpawl',\n",
    "             'ljjeL' : 'ljeL',\n",
    "             'ljjNkBurx' : 'ljNkBurx',\n",
    "             'TjjGmunt': 'TiGmunt',\n",
    "             'Jjjon' : 'Jjon',\n",
    "             'Jjjman' : 'Jjman',\n",
    "             'Jjje' : 'Jje', \n",
    "             'JjjBorx' : 'JjBorx', \n",
    "             'tSristoper' : 'kristoper', \n",
    "             'tSrome' : 'krome',\n",
    "             'tSristos' : 'kristos', \n",
    "             'tSrist' : 'krist', \n",
    "             'tSristjan' : 'kristjan',\n",
    "             'tSrisje' : 'krisje', \n",
    "             'tSronikles' : 'kronikles',\n",
    "             'tSristofer' : 'kristofer', \n",
    "             'tSronikle' : 'kronikle', \n",
    "             'tSristjane' : 'kristjane',\n",
    "             'tSrjjzler' : 'krizler', \n",
    "             'tSristmas' : 'kristmas', \n",
    "             'tSristje' : 'kristje', \n",
    "             'tSristina' : 'kristina', \n",
    "             'tSrister' : 'krister', \n",
    "             'tSristensen' : 'kristensen', \n",
    "             'tSris' : 'kris', \n",
    "             'tSristine' : 'kristine', \n",
    "             'tSromjun' : 'kromjun',\n",
    "             'tSristop' : 'kristop', \n",
    "             'tSristi' : 'kristi',\n",
    "             'Sperja' : 'eSperja',\n",
    "             'TrDaf' : 'serDaf',\n",
    "             'tjjBurn' : 'tiBurn',\n",
    "             'dJa' : 'deJa',\n",
    "             'btamer' : 'botamer',\n",
    "             'kjjlj' : 'kilj',\n",
    "             'dwjj' : 'dwier',\n",
    "             'gman' : 'geman',\n",
    "             'gmina' : 'gemina',\n",
    "             'gjjmnasjun' : 'gimnasjun',\n",
    "             'mLe' : 'madmosel',\n",
    "             'tmoBile' : 'temoBile',\n",
    "             'kmara' : 'kamara',\n",
    "             'kmart' : 'komart',\n",
    "             'kmer' : 'komer',\n",
    "             'tjjne' : 'tine',\n",
    "             'tSlons' : 'klons',\n",
    "             'tSloe' : 'kloe',\n",
    "             'tBer' : 'teBer',\n",
    "             'tBjjnoBelas' : 'teBinoBelas',\n",
    "             'xnkpiNg' : 'dZonkopiN',\n",
    "             'gjjeoNgsaNg' : 'gjeoNgsaNg',\n",
    "             'lxuBlxana' : 'ljuBljana',\n",
    "             'xpop' : 'keipop',\n",
    "             'krjjpton' : 'kripton', \n",
    "             'krjjptonjanos' : 'kriptonjanos', \n",
    "             'krjjpto': 'kripto',\n",
    "             'mtro' : 'metro',\n",
    "             'lnder' : 'linder',\n",
    "             'lnai' : 'lonai',\n",
    "             'sjjtSeDelik' : 'sitSeDelik',\n",
    "             'sjjtSo' : 'sitSo',\n",
    "             'dxaNgo' : 'dZaNgo'\n",
    "             \n",
    "             } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_train_df['phonl_tr'] = ES_train_df['phonl_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])\n",
    "ES_train_df['phont_tr'] = ES_train_df['phont_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])\n",
    "ES_test_df['phonl_tr'] = ES_test_df['phonl_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])\n",
    "ES_test_df['phont_tr'] = ES_test_df['phont_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])\n",
    "ES_val_df['phonl_tr'] = ES_val_df['phonl_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])\n",
    "ES_val_df['phont_tr'] = ES_val_df['phont_tr'].apply(fix_transcriptions, args=[ES_spelling, ES_abbr, ES_others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - lists should be empty\n",
    "\n",
    "ESacr_train = find_acronyms(ES_train_df) #195\n",
    "ESacr_test = find_acronyms(ES_test_df) #55\n",
    "ESacr_val = find_acronyms(ES_val_df) #37\n",
    "\n",
    "\n",
    "print(ESacr_train)\n",
    "print(ESacr_test)\n",
    "print(ESacr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed spanish corpus\n",
    "\n",
    "pkl_name = f\"fix_esCV_df.pkl\"\n",
    "with open(pkl_name, 'wb') as file:\n",
    "  # A new file will be created\n",
    "  pkl.dump([ES_train_df, ES_test_df, ES_val_df], file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "FR_train_df['phonl_tr'] = FR_train_df['phonl_tr'].apply(transcrCleanerMltLng, args=['fr'])\n",
    "FR_train_df['phont_tr'] = FR_train_df['phont_tr'].apply(transcrCleanerMltLng, args=['fr'])\n",
    "\n",
    "#test\n",
    "FR_test_df['phonl_tr'] = FR_test_df['phonl_tr'].apply(transcrCleanerMltLng, args=['fr'])\n",
    "FR_test_df['phont_tr'] = FR_test_df['phont_tr'].apply(transcrCleanerMltLng, args=['fr'])\n",
    "\n",
    "#val\n",
    "FR_val_df['phonl_tr'] = FR_val_df['phonl_tr'].apply(transcrCleanerMltLng, args=['fr'])\n",
    "FR_val_df['phont_tr'] = FR_val_df['phont_tr'].apply(transcrCleanerMltLng, args=['fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRacr_train = find_acronyms(FR_train_df) #280\n",
    "FRacr_test = find_acronyms(FR_test_df) #75\n",
    "FRacr_val = find_acronyms(FR_val_df) #81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of FR problematic trancsriptions\n",
    "\n",
    "FRall_probs = [el[1] for el in FRacr_test + FRacr_train + FRacr_val]\n",
    "FRall_probs_flat = [w for el in FRall_probs for w in el]\n",
    "\n",
    "\n",
    "# set of FR problematic transcriptions\n",
    "\n",
    "FRall_probs_s = list(set(FRall_probs_flat))\n",
    "# FRall_probs_s # 144 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language-dependent vocabularies to fix the issues\n",
    "\n",
    "FR_spelling = {'b' : 'be', \n",
    "               'B' : 'Be',\n",
    "               'tS' : 'tSe',\n",
    "               'ts' : 'tse', \n",
    "               'ttS' : 'tSe',\n",
    "               'd' : 'de',\n",
    "               'h' : 'aS',\n",
    "               'f' : 'ef',\n",
    "               'dZ' : 'dZe',\n",
    "               'Z' : 'Zi',\n",
    "               'S' : 'eS',\n",
    "               'dz' : 'dze',\n",
    "               'g' : 'Ze', \n",
    "               'j' : 'Zi',\n",
    "               'k' : 'ka',\n",
    "               'l' : 'el',\n",
    "               'm' : 'em',\n",
    "               'n' : 'en',\n",
    "               'p' : 'pe',\n",
    "               'q' : 'ku',\n",
    "               'R' : 'ER',\n",
    "               's' : 'es',\n",
    "               't' : 'te',            \n",
    "               'v' : 've',\n",
    "               'w' : 'dubleve',\n",
    "               'x' : 'iks',\n",
    "               'y' : 'igRek',\n",
    "               'z' : 'zed'}\n",
    "\n",
    "FR_abbr = {'mr' : 'm@sj2',\n",
    "           'ZR' : 'dZunior',\n",
    "           'dr' : 'dOktOr',\n",
    "           'vs' : 'vese',\n",
    "           'st' : 'se~',\n",
    "           'pp' : 'peaSpe',\n",
    "           'sf' : 'seef',\n",
    "           'gR' : 'gRam',\n",
    "           'msr': 'mo~sEJ9R',\n",
    "           'lt' : 'lj9t@na~',\n",
    "           'gd' : 'gra~',\n",
    "           'fR' :'fRa~',\n",
    "           'ps' : 'pese',\n",
    "           'kg': 'kilogRam',\n",
    "           'mt' : 'mo~',\n",
    "           'sgt' : 'sERZ@',\n",
    "           'mtR' : 'metR'}\n",
    "\n",
    "           \n",
    "\n",
    "FR_others = {'kt' : 'kE',\n",
    "             'klk ' : 'kelke',\n",
    "             'gf' : 'gof',\n",
    "             'lm' : 'aSelem',\n",
    "             'kRt' : 'kres',\n",
    "             'vksk ' : 'v2ty',\n",
    "             'lRm' : 'lerm',\n",
    "             'pR' : 'prE', \n",
    "             'kRn' : 'kRaon',\n",
    "             'pst ' : 'aSpeeste',\n",
    "             'snsd ' : 'seensedeaS',\n",
    "             'SJ' : 'SEJ@',\n",
    "             'bl' : 'bol@',\n",
    "             'tR' : 'tRu',\n",
    "             'kgpm' : 'seZepeem',\n",
    "             'dsS' : 'd2tS',\n",
    "             'fg' : 'fu',\n",
    "             'kw' : 'kawen',\n",
    "             'blk' : 'blOS',\n",
    "             'pksk' : 'p2ty',\n",
    "             'vlks' : 'volks',\n",
    "             'gfR' : 'gofR',\n",
    "             'ks' : 'iks',\n",
    "             'ft' : 'fest',\n",
    "             'njk' : 'nik',\n",
    "             'Zz' : 'Zani',\n",
    "             'pk' : 'pek',\n",
    "             'lw' : 'lo' ,\n",
    "             'dw' : 'dO',\n",
    "             'kld' : 'klaud',\n",
    "             'cefa' : 'sefa',\n",
    "             'cjedei' : 'sjedei',\n",
    "             'bZZR' : 'bjoRn',\n",
    "             'dZs' : 'deZees',\n",
    "             'tR' : 'tRent',\n",
    "             'stRw' : 'stRon',\n",
    "             'tm' : 'tEm',\n",
    "             'kn' : 'iksnee',\n",
    "             'dkst ' : 'duksy',\n",
    "             'sfb' : 'seefbe',\n",
    "             'gmb' : 'ZeembeaS',\n",
    "             'Rv' : 'R@v@',\n",
    "             'tt' : 'tEt@',\n",
    "             'sk' : 'sek',\n",
    "             'sdg' : 'sedeZe',\n",
    "             'Sw' : 'So~',\n",
    "             'bRdt' : 'bRo~nt',\n",
    "             'fw' : 'fo~n',\n",
    "             'gRv' : 'gr@v@',\n",
    "             'mm' : 'mEm@',\n",
    "             'wm' : 'woRms',\n",
    "             'nn' : 'nu',\n",
    "             'zn':'zane',\n",
    "             'tng':'to~',\n",
    "             'mlv' : 'mel@v',\n",
    "             'dpk' : 'dEpEs@',\n",
    "             'lbwE9~' : 'lOb@',\n",
    "             'dRnEk' : 'dREnEk',\n",
    "             'spjktOR' : 'spektOR',\n",
    "             'gRdplas' : 'gRa~dplas',\n",
    "             'bRnif' : 'bRanif',\n",
    "             'bRna~' : 'bRenan',\n",
    "             'bRnERtR' : 'bRynERtRot',\n",
    "             'tblEz' : 'se~blEz@',\n",
    "             'gRdpRe' : 'gR9~dpRe',\n",
    "             'gRdpiER' : 'gRa~dpiER',\n",
    "             'gRdpREsiJi' : 'gR9~dpREsiJi',\n",
    "             'gRdpER' : 'gRa~dpER',\n",
    "             'gRdp9R' : 'gR9~dp9R',\n",
    "             'gRdpaRadi' : 'gRa~dpaRadi',\n",
    "             'gRdpRi' : 'gR9~dpRi',\n",
    "             'dmoSygdo~gR2b': 'demSygdo~gR2b',\n",
    "             'dmoRaliz' : 'demoRaliz',\n",
    "             'dmHe~Ek' : 'demHe~Ek',\n",
    "             'mkRiEZ' : 'mEkRiEZ',\n",
    "             'RzitR' : 'R@tR',\n",
    "             'tRzik' : 'tRik',\n",
    "             'gmina' : 'gimina',\n",
    "             'gmini' : 'gimini',\n",
    "             'pRzityli' : 'pRSityli',\n",
    "             'gRabouo' : 'gRabovo',\n",
    "             'gRaZiwo' : 'kRajewo',\n",
    "             'Radzilo~e' : 'RatSilov',\n",
    "             'ekzykze~' : 'tSutSin',\n",
    "             'gdR9~' : 'gydR9~',\n",
    "             'dksEvR' : 'd9sEvR',\n",
    "             'dksia~se~ka~t' : 'dysa~se~ka~t',\n",
    "             'dkste' : 'd9kstje',\n",
    "             'dksia~' : 'd9ksja~',\n",
    "             'klna~bERg' : 'klejna~bERg',\n",
    "             'sZaak' : 'sjaak',\n",
    "             'mtOn' : 'metOn',\n",
    "             'mti' : 'meti',\n",
    "             'mtE' : 'metE',\n",
    "             'mtskle' : 'motkle',\n",
    "             'gfRe' : 'gofRe',\n",
    "             'fnap' : 'efenape',\n",
    "             'msz@RES' : 'medz@RES',\n",
    "             'szoZuR' : 'sa~ZuR',\n",
    "             'sziZ@ti' : 'zeg@ti',\n",
    "             'szymik' : 'symik',\n",
    "             'sztd2' : 'sa~td2',\n",
    "             'szy' : 'sy',\n",
    "             'dnag' : 'denaZe',\n",
    "             'dnwaje' : \"denwaje\",\n",
    "             'dna~toSi' : 'dena~toSi',\n",
    "             'dnipRop@tROvsk' : 'dnipRop@tROvsk',\n",
    "             'dniat' : 'deniat',\n",
    "             'Znlui' : 'Za~lwi',\n",
    "             'RZuisE' : 'ReZwisE',\n",
    "             'njwpOR' : 'njupORt',\n",
    "             'Zsom' : 'esom',\n",
    "             'pRsizjo~' : 'pRa~sizjo~',\n",
    "             'Rsi' : 'R@si',\n",
    "             'RpRima~d' : 'R@pRima~d',\n",
    "             'bRge' : 'bRogeR',\n",
    "             'kSuma~' : 'ESuma~',\n",
    "             'kSaRp' : 'ESaRp',\n",
    "             'pkS@Ri':'pESRi',\n",
    "             'kSEl' : 'ESEl',\n",
    "             'kSa~' : 'k2Sinsa~',\n",
    "             'dkSaRZ' : 'dESaRZ',\n",
    "             'nt9~dip' : 'nyt9~dip',\n",
    "             'nseh' : 'ense',\n",
    "             'tvERski' : 'tSvERski',\n",
    "             'tva' : 'tevea',\n",
    "             'tviJod' : 'diviJo~',\n",
    "             'gRdfRER' : 'gR9~dfRER',\n",
    "             'dmHe~Ek' : 'dEmHe~Ek',\n",
    "             'RuvRoi' : 'RuvRwa',\n",
    "             'blse~Em' : 'bolzenaim',\n",
    "             'dtaj' : 'd@taj',\n",
    "             'dt@ate' : 'det@ate',\n",
    "             'dtuRn' : 'detuRne',\n",
    "             'RvERi' : 'REvERi',\n",
    "             'kRkovnika' : 'kR@ kRov nika',\n",
    "             'gRdgijom' : 'gR9~dgijom',\n",
    "             'dgaZe' : 'degaZe',\n",
    "             'dgRada~' : 'degRada~',\n",
    "             'SJ2' : 'SEJ2',\n",
    "             'ZZe' : 'gegeR',\n",
    "             'ZZOn' : 'dZOn',\n",
    "             'ZZo~' : 'dZo~',\n",
    "             'mkZij' : 'mekgil',\n",
    "             'mkZin' : 'makgin',\n",
    "             'sksdmokORsE'  : 'skEdmokORsEt',\n",
    "             'skZEtE' : 'sZEtE',\n",
    "             'gfRe' : 'gofRe',\n",
    "             'gfE' : 'Sfe',\n",
    "             'ndoRo~boli' : ' endoRoemboli',\n",
    "             'ndiEj' : 'ndiEj',\n",
    "             'ndenu' : 'endenu',\n",
    "             'ndRiana' : 'EndRiana',\n",
    "             'dliag' : 'delig',\n",
    "             'dle~dis' : 'aldis',\n",
    "             'wt9~' : 'otin',\n",
    "             'jRviN' : 'iRviN',\n",
    "             'jRvin' : 'iRvin',\n",
    "             'jRo~g' : 'jERong',\n",
    "             'mjRwo2o' : 'mirvo',\n",
    "             'gdinia' : 'gdinja',\n",
    "             'gd9~' : 'gyda~',\n",
    "             'klmso~' : 'klemso~',\n",
    "             'sktRua~pf' : 'StRumpf',\n",
    "             'vlRi' : 'v9l@Ri',\n",
    "             'stZa~' : 'se~Za~',\n",
    "             'bORdkse~stZa~' : 'bORdokse~stZa~',\n",
    "             'ksStREl' : 'keStREl',\n",
    "             'swv9~k' : 'so~ntvik',\n",
    "             'fltSe' : 'flEtSe',\n",
    "             'lta~baseERtsEl' : 'l9ta~baXtsEl',\n",
    "             'ltSimi' : 'letSimi',\n",
    "             'RwgRa~' : 'RwagRa~',\n",
    "             'jld@S@Em' : 'ild@S@Em',\n",
    "             'zkRokaEli' : 'aizkRaukleR',\n",
    "             'gZalaRORn' : 'dZalaROR',\n",
    "             'gZo' : 'gyZo',\n",
    "             'ngZi' : 'engugi',\n",
    "             'gZa~mEstRa' : 'gyZa~mEstRa',\n",
    "             'gZe~Em': 'gugenaim',\n",
    "             'kRHiz@ko~tROl' : 'kRuzko~tROl',\n",
    "             'fpkodnbRouse' : 'peaSpekodbRuze',\n",
    "             'Rplika' : 'Replika',\n",
    "             'pskOv' : 'pskOv',\n",
    "             'tSjkOv' : 'tSikOv',\n",
    "             'tSjkova' : 'tSikova',\n",
    "             'bks@R' : 'byksjeR',\n",
    "             'bks@Z@R' : 'bofR@R@',\n",
    "             'bky' : 'beky',\n",
    "             'slno' : 's2lno',\n",
    "             'slnRi' : 's2lnRi',\n",
    "             'bmna' : 'beenema',\n",
    "             'mbuvm9~' : 'embuvma~', \n",
    "             'mbRiER' : 'bR9iER', \n",
    "             'mbEj' : 'embE',       \n",
    "             'mby' : 'embE2', \n",
    "             'mbOzomufu' : 'mybOzomufu', \n",
    "             'mbi' : 'mind', \n",
    "             'mbua~ba' :'mbumba', \n",
    "             'mbOzofulb' : 'mebOzofulb@', \n",
    "             'mbuiuZu' : 'mbuiuZu', \n",
    "             'mbatabyby' : 'embatabu',\n",
    "             'ZzdiN' : 'janedig',\n",
    "             'Zza~E' : 'ZanE',\n",
    "             'mkle~' : 'meklein' ,\n",
    "             'nsi@Ryzwij' : 'njed@Rzuwil',\n",
    "             'nsi@RERg@Em' : 'nid@RaeRgEm',\n",
    "             'nsiERmORsoSwiR' : 'nidERmORSwiR',\n",
    "             'nsEdaR' : 'niEdaR',\n",
    "             'Zdo' : 'Zedo~',\n",
    "             'gsle~gem@RinN' : 'geslingem@Rin',\n",
    "             'gRdpRe' : 'gRa~dpRe',\n",
    "             'mZa' : 'muntSak',\n",
    "             'Rn@k9~' : 'Ryn@k9~',\n",
    "             'Rna~ka~f' : 'Ryna~ka~f',\n",
    "             'fdsoSa~ko' : 'fedtSa~ko',\n",
    "             'bgOR' : 'bjORn',\n",
    "             'bgEja' : 'bEgEja',\n",
    "             'lgRma~' :  'leZeRma~',\n",
    "             'tsle' : 'EtsleR',\n",
    "             'RvRa~' : 'R@vRa~',\n",
    "             'tpa' : 'pa',\n",
    "             'sS@Em' : 'sySEm',\n",
    "             'ksSi' : 'kekSi',\n",
    "             'sSon@bEk' : 'Son@bEk',\n",
    "             'kbi' : 'kybits',\n",
    "             'tHSya' : 'tHeSa',\n",
    "             'pdy' : 'pedey',\n",
    "             'jvvSEn' : 'jevEn',\n",
    "             'm@lnika' : 'm@lniks',\n",
    "             'Rvolysjo~' : 'REvolysjo~',\n",
    "             'Rvlasjo~' : 'REvlasjo~',\n",
    "             'RvE' : 'REvE',\n",
    "             'gRtSEn' : 'gRetSEn',\n",
    "             'Rty' : 'eRtey',\n",
    "             'vRtZgba' : 'vERtojba',\n",
    "             'RtaS@te' : 'RtaS@te',\n",
    "             'Rte' : 'R9teR',\n",
    "             'wwolEbdodyva~dR@dikOm' : 'dubleve dubleve dubleve pwa~ lEbdodyva~dR@di pwa~ kOm',\n",
    "             'Swwal' : 'S9wal',\n",
    "             'wwyoS' : 'weitbRyS',\n",
    "             'Rgyz' : 'Regyse~',\n",
    "             'bRge' : 'boRgeR',\n",
    "             'mRw@t2@e' : 'm@sj2 w@teR',\n",
    "             'vga~' : \"woga~n\",\n",
    "             'kRme' : 'kR9me',\n",
    "             'svRma~' : 'sevRma~',\n",
    "             'gRlonz' : \"gRelen\",\n",
    "             'pRmiOm' : 'pREmiOm',\n",
    "             'pRvie~dREtil' : 'pRevie~dREtil',\n",
    "             'pRvot' : 'pRevo',\n",
    "             'ZnR@ne' : 'Zo~R@ne',\n",
    "             'ZnR@no' : 'Zo~nR@no',\n",
    "             'tkRiva~' : 't9kRiva~',\n",
    "             'ngliZama~' : 'negliZama~',\n",
    "             'Stkyti' : 'tSEkyti',\n",
    "             'gRdZa~' : 'gR9dZa~',\n",
    "             'Swwal' : 'Sawal',\n",
    "             'bRlEks' : 'bRo~',\n",
    "             'kfa' : 'cefa',\n",
    "             'kf@e' : 'kf@R',\n",
    "             'mjdwa~o' : 'midwest',\n",
    "             'tzz@a~dikobORda' : \"ekstzeandikoboRda\",\n",
    "             'Zma~liN' : 'gemaling',\n",
    "             'dZminamigoSi' : 'dodZominamigotSi',\n",
    "             'lgon' : 'l9gons',\n",
    "             'kstytEti' : 'kestytEkis',\n",
    "             'dkste' : 'd2ktSjeR',\n",
    "             'kstRayy' : 'ksaRaky',\n",
    "             'vkskRme' : 'v9kSoRme',\n",
    "             'mpRomE' : 'pRomE',\n",
    "             'mpala' : 'empala',\n",
    "             'pkS@Ri' : 'pES@Ri',\n",
    "             'sjkigaaRa' : 'sikigaaRa',\n",
    "             'kRwknORma~' : 'kRwanORma~',\n",
    "             'sRmoni' : 'tseRmoni',\n",
    "             'sRi2z' : 'seRi2z',\n",
    "             'sRinivaza~' : 'seRinivaza~',\n",
    "             'sRmski' : 'sRemski',\n",
    "             'sR@botnik' : 'zR@botnik',\n",
    "             'sRi2' : 'seRi2',\n",
    "             'sRi2zma~' : 'seRi2zma~',\n",
    "             'mdogo' : 'emdogo',\n",
    "             'md@s9~' : 'med@s9~',\n",
    "             'wma~' : 'uma~',\n",
    "             'stmoRis' : 'se~moRis',\n",
    "             'fRmis' : 'fRemis',\n",
    "             'SSEko' : 'SEko',\n",
    "             'dkSaRZ' : 'deSaRZe',\n",
    "             'sksdmokORsE' : 'sk2dmokORsE',\n",
    "             'bne' : 'bnej',\n",
    "             'Rpta' : 'Repeta',\n",
    "             'RptE' : 'RepetE',\n",
    "             'ksia~gZiaba' : 'zja~giaba',\n",
    "             'Sa~ge' : 'Sa~gaj',\n",
    "             'ksERtsi' : 'zERtiJi',\n",
    "             'ksinga~' : 'tSia~',\n",
    "             'ksio' : 'kzio',\n",
    "             'ksio~EJy' : 'kzjo~gy',\n",
    "             'kso~E' : 'kzjo~E',\n",
    "             'ksit' : 'gzEts',\n",
    "             'ksiiRi' : 'dizHtjEm',\n",
    "             'ksamE' : 'samaks',\n",
    "             'ksAE' : 'zw2~',\n",
    "             'Engi' : 'EngujEn',\n",
    "             'ksStREl' : 'keStR',\n",
    "             'kso' : 'se es o',\n",
    "             'ksEmm' : 've~gtjem',\n",
    "             'ksOzo~' : 'ksOzo~',\n",
    "             '@RiksOzo~' : '@Rikso~',\n",
    "             'ksOzo~' : 'akesson',\n",
    "             'ksavie' : 'eksavie',\n",
    "             'kstytEti' : 'kestytEtis',\n",
    "             'kskktOR' : 'iksfaktOR',\n",
    "             'ke@slo' : 'tSe@slo',\n",
    "             'ksipa' : 'zipas',\n",
    "             'ksiv' : 'kezjem',\n",
    "             ' ksE' : 'iksmEn',\n",
    "             'ksERv@niak' : 'SERv@niak',\n",
    "             'ksaR' : 'ksaR',\n",
    "             'zja~' : 'zja~',\n",
    "             'ksaZ' : 'zave',\n",
    "             'ksila~g' : 'dZiJo~nge',\n",
    "             'ksakiadaki' : 'ksalkiadakis',\n",
    "             'ksp@nOl' : 'kop@nOl',\n",
    "             'ks@Rnptlo~Zme' : 'z@Ruptlo~Zme',\n",
    "             'ksijEm' : 'dikseptjEm',\n",
    "             'kstRayy' : 'iksRay',\n",
    "             'sdaRmstadete' : 'esdaRmstat',\n",
    "             'bks@R' : 'byksjeR',\n",
    "             'gRdmER' : 'gRa~dmER',\n",
    "             'gRdesi' : 'gRa~desi',\n",
    "             'gRdvikER' : 'gRa~dvikER',\n",
    "             'gRdSa~' : 'gRa~dSa~',\n",
    "             'gRdpREsiJi' : 'gRa~dpREsiJi',\n",
    "             'gRdmEzo~' : 'gRa~dmEzo~',\n",
    "             'gRdpRi' : 'gRa~dpRi',\n",
    "             'gRdmaitR' : 'gRa~dmaitR',\n",
    "             'gRdZa~' : 'gRa~dZa~',\n",
    "             'gRdSoz' : 'gRa~dSoz',\n",
    "             'gRdvEl' : 'gRa~dvEl',\n",
    "             'gRdmEs' : 'gRa~dmEs',\n",
    "             'gRdgijom' : 'gRa~dgijom',\n",
    "             'gRdval' : 'gRa~dval',\n",
    "             'gRdSav9~' : 'gRa~dSav9~',\n",
    "             'dsEv' :'djuvs',\n",
    "             'kdsa~t@n2f' : 'd9ksa~t@n2f',\n",
    "             'ds@go' : 'djEgo',\n",
    "             'dsd@Rika' : 'dj2d@Rik',\n",
    "             'gbEj' : 'ZbEj',\n",
    "             'ttky' : 'tajtoky',\n",
    "             'fpyni' : 'paSpeyni',\n",
    "             'vSikyl': 'veikl',\n",
    "             'zzbRid' : 'zbRid',\n",
    "             'pta' : ' peta',\n",
    "             'ptSoRa' : 'petSoRa',\n",
    "             'pteRobRa~S' : 'pteRobRa~S',\n",
    "             'ptSa~ga' : 'petSa~ga',\n",
    "             'pti' : 'pti',\n",
    "             'mJa~' : 'miJa~',\n",
    "             'fpkodnbRouse' : 'peSpek9dbRouse',\n",
    "             'ZSalysin' : 'Zalysin',\n",
    "             'ZSinEsykgE' : 'ZaSin Esykoge~',\n",
    "             'ZSazitE' : 'Zezata~',\n",
    "             'ZSazit' : 'Zezit',\n",
    "             'ZSabijE' : 'ZabijE',\n",
    "             'ZSabit' : 'Zabit',\n",
    "             'bmna' : 'be emena',\n",
    "             'mno' : 'emeno',\n",
    "             'bZaRk':'bjaRk',\n",
    "             'nkwa' : 'nikoi',\n",
    "             'kta' : 'kekta',\n",
    "             'kti' : 'setei',\n",
    "             'kt9R' : 'st9R',\n",
    "             'sktRua~pf' : 'StRumpf',\n",
    "             'pljlist' : 'plilist',\n",
    "             'dfa~dR' : 'defa~dR',\n",
    "             'dfEt' : 'defEt',\n",
    "             'mlia' : 'mia mlja',\n",
    "             'pomla~Z@v9~' : 'pola~Z@v9~',\n",
    "             'mla~Z' : 'mela~Z',\n",
    "             'mla~kai' : 'mela~kai',\n",
    "             'mlila' : 'emlila',\n",
    "             'mlok' : 'emlok',\n",
    "             'lvOv' : 'elvOv',\n",
    "             'lviv' : 'elviv',\n",
    "             'gzilofaZ' : 'zilofaZ',\n",
    "             'gzou' : 'zou',\n",
    "             'gza~zy' : 'zia~zy',\n",
    "             'gzim' : 'igzim',\n",
    "             'gzii' : 'tRezje',\n",
    "             'gz@n' : 'Xim@nes',\n",
    "             'mlz9~k' : 'melnik',\n",
    "             'mke~' : 'meke~',\n",
    "             'mkakasi' : 'kakatsi',\n",
    "             'mkoaRti' : 'mekoaRti',\n",
    "             'mkoaRti' : 'mekoaRti',\n",
    "             'mkZij' : 'makZij',\n",
    "             'mkylk' : 'makylk',\n",
    "             'mkuR' : 'mekuR',\n",
    "             'mku' : 'emku',\n",
    "             'mkgR@gOR' : 'makgR@gOR',\n",
    "             'mkle~' : 'mekle~',\n",
    "             'mkZin' : 'ma gin',\n",
    "             'mkolga~' : 'makolga~',\n",
    "             'mka~n' : 'meka~n',\n",
    "             'mkRiEZ' : 'makRiEZ',\n",
    "             'mkasj' : 'makasj',\n",
    "             'mkgRu' : 'mekgRu',\n",
    "             'Rpo~dizZ' : 'Repo~dizZ',\n",
    "             'Rpo~diR' : 'Repo~diR',\n",
    "             'Rpo~dRe' : 'Repo~dRe',\n",
    "             'Rpo~' : 'Repo~',\n",
    "             'Rpo~ditil' : 'Repo~ditil',\n",
    "             'Rpo~dE' : 'Repo~dE',\n",
    "             'Rpo~di' : 'Repo~di',\n",
    "             'Rpo~s' : 'Repo~s',\n",
    "             'RpRima~d' : 'RepRima~d',\n",
    "             'RpaR' : 'RepaR',\n",
    "             'Rpo~dy' : 'Repo~dy',\n",
    "             'fdZEld' : 'fjEld',\n",
    "             'tle~Zi'  :'tingits',\n",
    "             'tlaba' : 'telaba',\n",
    "             'tlaSyak' : 'telyak',\n",
    "             'tlepolEm' : 'tlepolEm',\n",
    "             'gvis' : 'gwis',\n",
    "             'gva' : 'gwa',\n",
    "             'gve' : 'gwe~',\n",
    "             'kvat@Rnik' : 'ekvat@Rnik',\n",
    "             'kviaskOvski' : 'ekviaskOvski',\n",
    "             'kvinikideE' : 'kvinikideE',\n",
    "             'ngZi' : 'engugi',\n",
    "             'gmaj' : 'dZimaj',\n",
    "             'Rklam@REtil' : 'Reklam@REtil',\n",
    "             'Rkolt' : 'Rekolt',\n",
    "             'mjlwoks' : 'milwoks',\n",
    "             'tsta~' : 'tseta~',\n",
    "             'dklaR' : 'deklaR',\n",
    "             'dkavj' : 'd9vil',\n",
    "             'dkate~tkwatR' : 'd9ksviZ',\n",
    "             'dkSaRZ' : 'deSaRZe',\n",
    "             'vRZaka' : 'vZaRka',\n",
    "             'mtskle' : 'm2tskle',\n",
    "             'dbatE' : 'debatE',\n",
    "             'dbaRas' : 'debaRas',\n",
    "             'dgRada~' : 'degRada~',\n",
    "             'ngRam' : 'engRam',\n",
    "             'googl' : \"gugol\",\n",
    "             'ngR@la' : 'negR@la',\n",
    "             'vkskROR': 'v9skROR',\n",
    "             'SwtataR' : 'S9StataR',\n",
    "             'tsgyk@maR' : 'dZyk@maR',\n",
    "             'bpifRa~s' : 'bepifRa~s',\n",
    "             'bpi' : 'bepi',\n",
    "             'kRwksonaR' : 'kRwadonoR',\n",
    "             'kzpi' : 'Sopi',\n",
    "             'bwtie' : 'bwatje',\n",
    "             'kda~Sa' : \"kode~Sa\",\n",
    "             'kdsa~t@n2f' : 'tRo~t@n2f',\n",
    "             'kdi' : 'cjedei',\n",
    "             'pRsizma~': 'pResizjo~',\n",
    "             'ZtynEmaR' : 'ZotynEmaR',\n",
    "             'tk@S@laSvili' : 'tek@S@laSvili',\n",
    "             'tki' : 'toki',\n",
    "             'tkEd' : 'tokEd',\n",
    "             'ZRalma~' : 'ZeRalma~',\n",
    "             'ZRal' : 'Ze~Ral',\n",
    "             'ZR@vEl' : 'ZeR@vEl',\n",
    "             'ZRi' : 'ZeRi',\n",
    "             'SksftuR' : 'S2fuR',\n",
    "             'dv9~' : 'dov9~',\n",
    "             'dv@lOp' : 'dev@lOp',\n",
    "             'lREt' : 'l2REt',\n",
    "             'vnie' : 'vinje',\n",
    "             'nguabi' : 'enguabi',\n",
    "             'ngaRikORskm' : 'gaRikORsum',\n",
    "             'ngazidZa' : \"engazitSa\",\n",
    "             'ngliZama~' : 'engliZama~',\n",
    "             'ngekou' : 'engekou',\n",
    "             'ngliZa~s' : 'negliZa~s',\n",
    "             'nnaZEn' : 'ennaZEn',\n",
    "             'tbilisi' : 'tbelisi',\n",
    "             'stfanik' : 'stefanik',\n",
    "             'bziktaz' : 'beZitas',\n",
    "             'tmwe~' : 'temo~',\n",
    "             'bfR@mo~' : 'bofR@mo~',\n",
    "             'kRwkRtuZ' : 'kRwaRuZ',\n",
    "             'mbRiER' : 'bRejER',\n",
    "             'vzliN' : 'vazliN',\n",
    "             'fmin9~' : 'femin9~',\n",
    "             'ljkaEno' : 'lekno',\n",
    "             'btyl' : 'betyl',\n",
    "             'bt@sla' : 'betola',\n",
    "             'bta~do~' : 'beta~do~',\n",
    "             'bta~kuR' : 'beta~kuR',\n",
    "             'npRE' : 'nepRE',\n",
    "             'Znbatist' : 'Za~batist',\n",
    "             'ZnbatistESaRl' : 'Za~batistESaRl',\n",
    "             'JnagboduJou@' : 'gnagboduJou@',\n",
    "             'Sjlw@tHSap' : 'Silw@tHSap',\n",
    "             'nko~' : 'nko~',\n",
    "             'mssig' : 'metsik'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_train_df['phonl_tr'] = FR_train_df['phonl_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])\n",
    "FR_train_df['phont_tr'] = FR_train_df['phont_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])\n",
    "FR_test_df['phonl_tr'] = FR_test_df['phonl_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])\n",
    "FR_test_df['phont_tr'] = FR_test_df['phont_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])\n",
    "FR_val_df['phonl_tr'] = FR_val_df['phonl_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])\n",
    "FR_val_df['phont_tr'] = FR_val_df['phont_tr'].apply(fix_transcriptions, args=[FR_spelling, FR_abbr, FR_others])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - lists should be empty\n",
    "\n",
    "FRacr_train = find_acronyms(FR_train_df) #195\n",
    "FRacr_test = find_acronyms(FR_test_df) #55\n",
    "FRacr_val = find_acronyms(FR_val_df) #37\n",
    "\n",
    "\n",
    "print(FRacr_train)\n",
    "print(FRacr_test)\n",
    "print(FRacr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fixed french corpus\n",
    "\n",
    "pkl_name = f\"fix_frCV_df.pkl\"\n",
    "with open(pkl_name, 'wb') as file:\n",
    "  # A new file will be created\n",
    "  pkl.dump([FR_train_df, FR_test_df, FR_val_df], file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add transcriptions to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grxphoMAUS(dataset, transcr_df):\n",
    "  \n",
    "  \n",
    "  \"\"\" creates a list with the phonological transcriptions\"\"\"\n",
    "  all_transcribed_s = [ ]\n",
    "\n",
    "  for batch in dataset:\n",
    "    b_sent = batch['sentence']\n",
    "    b_audio = batch['path']\n",
    "    filen = os.path.split(b_audio)[-1] # getting rid of path\n",
    "    filen_ok = os.path.splitext(filen)[0] # getting rid of ext\n",
    "\n",
    "    pho_sent = transcr_df.loc[transcr_df['filename'] == filen_ok, 'phonl_tr']\n",
    "    pho_sent_list = pho_sent.to_list()\n",
    "    pho_sentence = pho_sent_list[0]\n",
    "    all_transcribed_s.append(pho_sentence)\n",
    "\n",
    "  return all_transcribed_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*-------- starting IT transcriptions --------*')\n",
    "\n",
    "with open('fix_itCV_df.pkl', 'rb') as file:\n",
    "  IT_train_df, IT_test_df, IT_val_df = pkl.load(file)\n",
    "\n",
    "\n",
    "transcriptions_trainIT = grxphoMAUS(common_voice_trainIT, IT_train_df)\n",
    "transcriptions_testIT = grxphoMAUS(common_voice_testIT, IT_test_df)\n",
    "transcriptions_valIT = grxphoMAUS(common_voice_validationIT, IT_val_df)\n",
    "\n",
    "common_voice_trainIT = common_voice_trainIT.add_column(\"phonl_tr\", transcriptions_trainIT)\n",
    "common_voice_testIT = common_voice_testIT.add_column(\"phonl_tr\", transcriptions_testIT)\n",
    "common_voice_validationIT = common_voice_validationIT.add_column(\"phonl_tr\", transcriptions_valIT)\n",
    "\n",
    "\n",
    "print('*------- sentence transcriptions done -------*')\n",
    "\n",
    "## ---------------- DOWNSAMPLING \n",
    "\n",
    "print('*------- downsampling 48kHz -> 16kHz -------*')\n",
    "\n",
    "common_voice_trainIT = common_voice_trainIT.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_testIT = common_voice_testIT.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_validationIT = common_voice_validationIT.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "print(common_voice_trainIT[45])\n",
    "print(common_voice_testIT[36])\n",
    "print(common_voice_validationIT[23])\n",
    "\n",
    "\n",
    "print('*------- IT dataset ready! -------*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*-------- starting ES transcriptions --------*')\n",
    "\n",
    "\n",
    "with open('fix_esCV_df.pkl', 'rb') as file:\n",
    "  ES_train_df, ES_test_df, ES_val_df = pkl.load(file)\n",
    "\n",
    "\n",
    "transcriptions_trainES = grxphoMAUS(common_voice_trainES, ES_train_df)\n",
    "transcriptions_testES = grxphoMAUS(common_voice_testES, ES_test_df)\n",
    "transcriptions_valES = grxphoMAUS(common_voice_validationES, ES_val_df)\n",
    "\n",
    "common_voice_trainES = common_voice_trainES.add_column(\"phonl_tr\", transcriptions_trainES)\n",
    "common_voice_testES = common_voice_testES.add_column(\"phonl_tr\", transcriptions_testES)\n",
    "common_voice_validationES = common_voice_validationES.add_column(\"phonl_tr\", transcriptions_valES)\n",
    "\n",
    "\n",
    "print('*------- sentence transcriptions done -------*')\n",
    "\n",
    "## ---------------- DOWNSAMPLING \n",
    "\n",
    "print('*------- downsampling 48kHz -> 16kHz -------*')\n",
    "\n",
    "common_voice_trainES = common_voice_trainES.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_testES = common_voice_testES.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_validationES = common_voice_validationES.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "print(common_voice_trainES[-1])\n",
    "print(common_voice_testES[-1])\n",
    "print(common_voice_validationES[-2])\n",
    "\n",
    "\n",
    "print('*------- ES dataset ready! -------*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*-------- starting FR transcriptions --------*')\n",
    "\n",
    "with open('fix_frCV_df.pkl', 'rb') as file:\n",
    "  FR_train_df, FR_test_df, FR_val_df = pkl.load(file)\n",
    "\n",
    "transcriptions_trainFR = grxphoMAUS(common_voice_trainFR, FR_train_df)\n",
    "transcriptions_testFR = grxphoMAUS(common_voice_testFR, FR_test_df)\n",
    "transcriptions_valFR = grxphoMAUS(common_voice_validationFR, FR_val_df)\n",
    "\n",
    "common_voice_trainFR = common_voice_trainFR.add_column(\"phonl_tr\", transcriptions_trainFR)\n",
    "common_voice_testFR = common_voice_testFR.add_column(\"phonl_tr\", transcriptions_testFR)\n",
    "common_voice_validationFR = common_voice_validationFR.add_column(\"phonl_tr\", transcriptions_valFR)\n",
    "\n",
    "\n",
    "print('*------- sentence transcriptions done -------*')\n",
    "\n",
    "## ---------------- DOWNSAMPLING \n",
    "\n",
    "print('*------- downsampling 48kHz -> 16kHz -------*')\n",
    "\n",
    "common_voice_trainFR = common_voice_trainFR.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_testFR = common_voice_testFR.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_validationFR = common_voice_validationFR.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "print(common_voice_trainFR[-1])\n",
    "print(common_voice_testFR[-1])\n",
    "print(common_voice_validationFR[-2])\n",
    "\n",
    "\n",
    "print('*------- FR dataset ready! -------*')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the multilingual corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_trainMLitesfr = concatenate_datasets([common_voice_trainIT, common_voice_trainES, common_voice_trainFR])\n",
    "common_voice_testMLitesfr = concatenate_datasets([common_voice_testIT, common_voice_testES, common_voice_testFR])\n",
    "common_voice_validationMLitesfr = concatenate_datasets([common_voice_validationIT, common_voice_validationES, common_voice_validationFR])\n",
    "\n",
    "print(f'number of files: TRAIN: {len(common_voice_trainMLitesfr)}, TEST:  {len(common_voice_testMLitesfr)}, VAL: {len(common_voice_validationMLitesfr)}')\n",
    "\n",
    "\n",
    "MLlen_post_TR = computeTotLen(common_voice_trainMLitesfr)\n",
    "MLlen_post_TST = computeTotLen(common_voice_testMLitesfr)\n",
    "MLlen_post_VAL = computeTotLen(common_voice_validationMLitesfr)\n",
    "\n",
    "print(f'ML dataset len in sec post filter TRAIN: {common_voice_trainMLitesfr} - TEST: {common_voice_testMLitesfr} - VAL {common_voice_validationMLitesfr}')\n",
    "\n",
    "# save the multilingual corpus in a pkl\n",
    "\n",
    "pkl_name = f\"MLitesfrCVdataset7_transcribed20.pkl\"\n",
    "with open(pkl_name, 'wb') as file:\n",
    "  # A new file will be created\n",
    "  pkl.dump([common_voice_trainMLitesfr, common_voice_testMLitesfr, common_voice_validationMLitesfr], file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cb8d6a7a1ec48364c58efa374d1a4f5aba50f1a80848d19fa6d9ee12721c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
